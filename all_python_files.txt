=== ./launcher.py ===
# launcher.py
import sys
from pathlib import Path

def app_root() -> Path:
    """Return path where bundled files live (PyInstaller _MEIPASS) or project root."""
    if getattr(sys, "frozen", False) and hasattr(sys, "_MEIPASS"):
        return Path(sys._MEIPASS)
    return Path(__file__).resolve().parent

ROOT = app_root()
APP_PATH = ROOT / "streamlit_app" / "app.py"

if not APP_PATH.exists():
    print("Erreur: l'application Streamlit introuvable à", APP_PATH)
    sys.exit(1)

# emulate CLI: streamlit run <app.py> --server.headless true --server.port 8501
sys.argv = ["streamlit", "run", str(APP_PATH), "--server.headless", "true", "--server.port", "8501"]

# import the streamlit CLI entrypoint
try:
    # streamlit >=1.0
    from streamlit.web import cli as stcli
except Exception:
    try:
        from streamlit import cli as stcli
    except Exception:
        stcli = None

if not stcli:
    print("Impossible d'importer streamlit. Vérifiez l'environnement.")
    sys.exit(1)

# Run Streamlit
sys.exit(stcli.main())
=== ./dist/launcher/_internal/streamlit_app/app.py ===
# streamlit_app/app.py
# Interface Streamlit pour l'extracteur PDF SWIFT
import sys
from pathlib import Path
import tempfile
import shutil
import io
import traceback

import streamlit as st
import pandas as pd

# --- make project root importable and prefer 'backend' package ---
ROOT = Path(__file__).resolve().parents[1]   # project root: pdf-extractor
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

# import backend functions (extractor manager)
try:
    # import extract_single, create_workbook and extract_dispatch in one place
    from backend.app.extractor_manager import extract_single, create_workbook, extract_dispatch
except Exception as e:
    st.error(f"Impossible d'importer l'extracteur backend: {e}")
    st.stop()

from backend.app.extractors import bic_utils

try:
    m = bic_utils.load_bic_mapping()    # retourne dict
    st.write("BIC map size:", len(m))
except Exception as e:
    st.write("BIC load failed:", e)

# UI configuration
st.set_page_config(page_title="PDF SWIFT Extractor (GUI)", layout="wide")
st.title("PDF SWIFT Extractor — Interface clic-clic")

st.markdown(
    """
    **Mode d'emploi rapide**
    - Glisser-déposer un ou plusieurs fichiers PDF ci-dessous.
    - Cliquez sur **Extraire**. Les fichiers sont analysés localement.
    - Téléchargez le workbook Excel ou enregistrez-le sur le serveur.
    """
)

# File uploader (multiple)
uploaded_files = st.file_uploader("Choisir des fichiers PDF", type="pdf", accept_multiple_files=True)

col1, col2 = st.columns([1, 1])
with col1:
    save_mode = st.radio("Mode de sortie", ("Télécharger le workbook", "Enregistrer sur le serveur (output/tables)"))

with col2:
    custom_out = st.text_input("Chemin de sortie (optionnel pour enregistrement serveur)", value=str(ROOT / "output" / "tables"))

run_button = st.button("Extraire")

# Logs viewer
with st.expander("Afficher les derniers logs"):
    log_file = ROOT / "logs" / "app.log"
    if log_file.exists():
        try:
            txt = log_file.read_text(encoding="utf-8")
            lines = txt.strip().splitlines()[-400:]
            st.text_area("logs/app.log (tail)", value="\n".join(lines), height=300)
        except Exception as e:
            st.write("Impossible de lire le fichier de logs:", e)
    else:
        st.write("Aucun fichier de logs trouvé (logs/app.log).")

# helper: save uploaded file to temp path and return Path
def save_uploaded_to_temp(uploaded) -> Path:
    tmpdir = Path(tempfile.mkdtemp(prefix="pdf_extr_"))
    dest = tmpdir / uploaded.name
    with open(dest, "wb") as f:
        f.write(uploaded.getbuffer())
    return dest

if run_button:
    if not uploaded_files:
        st.warning("Aucun fichier sélectionné.")
    else:
        rows = []
        progress = st.progress(0)
        total = len(uploaded_files)
        idx = 0
        errors = []
        tmp_dirs = []  # to cleanup later
        st.info(f"Lancement de l'extraction pour {total} fichier(s)...")

        for uf in uploaded_files:
            idx += 1
            st.write(f"Traitement : **{uf.name}** ({idx}/{total})")
            try:
                tmp_path = save_uploaded_to_temp(uf)
                tmp_dirs.append(tmp_path.parent)
            except Exception as e:
                errors.append((uf.name, f"Impossible d'enregistrer temporairement: {e}"))
                st.error(f"Impossible d'enregistrer temporairement {uf.name}: {e}")
                progress.progress(int(idx / total * 100))
                continue

            try:
                # extract_dispatch retourne toujours une LISTE de rows (1 ou plusieurs)
                new_rows = extract_dispatch(tmp_path)

                # Normalisations/garanties : mêmes clés pour chaque row, et source_pdf bien renseigné
                for r in new_rows:
                    # garantir la clé 'beneficiaire'
                    if "beneficiaire" not in r:
                        r["beneficiaire"] = None

                    # mapping backward-compatible pour 'donneur_dordre' si l'extracteur a renvoyé 'institution_name'
                    if "donneur_dordre" not in r:
                        if "institution_name" in r and r["institution_name"]:
                            r["donneur_dordre"] = r.get("institution_name")
                        else:
                            r["donneur_dordre"] = None

                    # s'assurer d'un source_pdf correct :
                    # - si l'extracteur n'a pas rempli source_pdf (rare), utiliser le nom du fichier uploadé
                    if not r.get("source_pdf"):
                        r["source_pdf"] = uf.name

                    # pour sécurité, si type_MT est None, on met un placeholder
                    if not r.get("type_MT"):
                        r["type_MT"] = None

                    rows.append(r)

                # message utilisateur synthétique
                types = sorted({rr.get("type_MT") or "type inconnu" for rr in new_rows})
                st.success(f"OK : {len(new_rows)} message(s) traité(s) — types : {', '.join(types)}")

            except Exception as e:
                tb = traceback.format_exc()
                errors.append((uf.name, str(e)))
                st.error(f"Erreur pendant l'extraction de {uf.name} : {e}")
                # affichage du traceback pour debug (expandable)
                with st.expander(f"Détails erreur pour {uf.name}"):
                    st.text(tb)
            progress.progress(int(idx / total * 100))

        # cleanup temp dirs
        for d in tmp_dirs:
            try:
                shutil.rmtree(d, ignore_errors=True)
            except Exception:
                pass

        progress.empty()

        # assemble display DataFrame (map internal keys -> user-facing labels)
        if rows:
            display_rows = []
            for r in rows:
                display_rows.append({
                    "code_banque": r.get("code_banque"),
                    "date_reference": r.get("date_reference"),
                    "reference": r.get("reference"),
                    "type_MT": r.get("type_MT"),
                    "pays_iso3": r.get("pays_iso3"),
                    # use the new key extracted by mt_multi
                    "donneur d'ordre": r.get("donneur_dordre"),
                    "Bénéficiaire": r.get("beneficiaire"),
                    "montant": r.get("montant"),
                    "devise": r.get("devise"),
                    "source_pdf": r.get("source_pdf")
                })

            df = pd.DataFrame(display_rows)

            # format montant column for display (no permanent change to rows)
            if "montant" in df.columns:
                try:
                    df["montant"] = df["montant"].apply(lambda x: ("{:,}".format(x)).replace(",", " ") if pd.notnull(x) else x)
                except Exception:
                    pass

            st.success("Extraction terminée — aperçu ci-dessous")
            st.dataframe(df, use_container_width=True)

            # Ensure backward-compatibility: create_workbook expects 'institution_name'
            for r in rows:
                if not r.get("institution_name"):
                    r["institution_name"] = r.get("donneur_dordre")

            # create workbook and either offer download or save on server
            if save_mode == "Télécharger le workbook":
                # create workbook in a temp directory and provide download
                temp_outdir = Path(tempfile.mkdtemp(prefix="swift_out_"))
                try:
                    out_path = create_workbook(rows, temp_outdir)  # returns Path to created workbook
                    with open(out_path, "rb") as f:
                        data = f.read()
                    st.download_button(
                        label="Télécharger le workbook Excel",
                        data=data,
                        file_name=out_path.name,
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                    )
                    st.info(f"Workbook généré: {out_path.name} (temp)")
                except Exception as e:
                    st.error(f"Impossible de créer le workbook: {e}")
                finally:
                    # optional: remove temp_outdir after offering download (download keeps data in browser)
                    try:
                        shutil.rmtree(temp_outdir, ignore_errors=True)
                    except Exception:
                        pass
            else:
                # save on server (custom_out or default)
                outdir = Path(custom_out) if custom_out else (ROOT / "output" / "tables")
                outdir.mkdir(parents=True, exist_ok=True)
                try:
                    outpath = create_workbook(rows, outdir)
                    st.success(f"Workbook enregistré : {outpath}")
                    st.write("Fichiers présents dans", outdir)
                    st.write(sorted([p.name for p in outdir.glob("*.xlsx")], reverse=True))
                except Exception as e:
                    st.error(f"Impossible d'enregistrer le workbook sur le serveur: {e}")

        else:
            st.warning("Aucun résultat extrait. Vérifiez le format des PDFs ou les logs.")

        # show errors list if any
        if errors:
            st.markdown("### Erreurs rencontrées")
            for name, msg in errors:
                st.write(f"- **{name}** : {msg}")

        st.info("Opération terminée.")
=== ./dist/launcher/_internal/backend/app/main.py ===
# backend/app/main.py
from fastapi import FastAPI
from .api import router as api_router

app = FastAPI(title="PDF Swift Extractor API", version="0.1")
app.include_router(api_router, prefix="")
=== ./dist/launcher/_internal/backend/app/db.py ===
# backend/app/db.py  (pbkdf2_sha256 backend to avoid bcrypt binary issues)
import os
from sqlalchemy import create_engine, Column, Integer, String
from sqlalchemy.orm import sessionmaker, declarative_base
from passlib.context import CryptContext

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATABASE_URL = os.environ.get("DATABASE_URL", f"sqlite:///{os.path.join(BASE_DIR, 'app.db')}")

engine = create_engine(DATABASE_URL, connect_args={"check_same_thread": False} if DATABASE_URL.startswith("sqlite") else {})
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()

# Use pbkdf2_sha256 (pure-python, no bcrypt C extension needed)
pwd_context = CryptContext(schemes=["pbkdf2_sha256"], deprecated="auto")

class User(Base):
    __tablename__ = "users"
    id = Column(Integer, primary_key=True, index=True)
    username = Column(String, unique=True, index=True, nullable=False)
    hashed_password = Column(String, nullable=False)
    role = Column(String, default="user")

def init_db():
    Base.metadata.create_all(bind=engine)

def get_user(db, username: str):
    return db.query(User).filter(User.username == username).first()

def create_user(db, username: str, password: str, role: str = "user"):
    # truncate very long passwords defensively
    if password is None:
        password = ""
    if isinstance(password, str):
        pw = password
    else:
        pw = str(password)
    # pbkdf2 supports long passwords; but keep reasonable length
    pw = pw[:1024]
    hashed = pwd_context.hash(pw)
    user = User(username=username, hashed_password=hashed, role=role)
    db.add(user)
    db.commit()
    db.refresh(user)
    return user

def verify_password(plain, hashed):
    return pwd_context.verify(plain, hashed)
=== ./dist/launcher/_internal/backend/app/extractor_manager.py ===
# backend/app/extractor_manager.py
import os
import sys
from pathlib import Path
import re
import logging
from datetime import datetime
from typing import List, Dict, Optional

from openpyxl import Workbook
from openpyxl.utils import get_column_letter

from .utils import logger

# import extractors (primary helpers)
from .extractors.mt202 import extract_for_mt202, extract_text_from_pdf as extract_text_mt202
try:
    from .extractors.mt103 import extract_for_mt103
    HAS_MT103 = True
except Exception:
    HAS_MT103 = False
    logger.info("mt103 extractor not available at import time; you can add it to EXTRACTOR_MAP later.")

try:
    from .extractors.mt910 import extract_for_mt910
    HAS_MT910 = True
except Exception:
    HAS_MT910 = False
    logger.info("mt910 extractor not available at import time.")

# try to import the multi-message extractor (optional)
try:
    from .extractors import mt_multi as mt_multi_module
    HAS_MT_MULTI = True
except Exception:
    mt_multi_module = None
    HAS_MT_MULTI = False
    logger.info("mt_multi extractor not available at import time; multi-file detection will fall back to single extractors.")

# regex to detect MT/FIN code
MT_DETECT = re.compile(r'\b(?:MT|FIN)[\s\-\_\.:\/]*(\d{3})\b', re.I)
IDENTIFIER_FIN_RE = re.compile(r'Identifier[:\s]*fin[\.:\s\-\/]*(\d{3})', re.I)

# map MT number -> extractor callable
EXTRACTOR_MAP = {
    "202": extract_for_mt202,
}
if HAS_MT103:
    EXTRACTOR_MAP["103"] = extract_for_mt103
if HAS_MT910:
    EXTRACTOR_MAP["910"] = extract_for_mt910

# -----------------------------
# BIC mapping / donor logic
# -----------------------------
# caching globals
_cached_mapping: Optional[Dict[str, str]] = None
_cached_mapping_path: Optional[str] = None

# heuristics: possible default file locations
_DEFAULT_XLS_PATHS = [
    "data/bic_codes.xlsx",
    "data/bic.xlsx",
    "bic_codes.xlsx",
    "bic.xlsx",
    "data/bfde98b8-0a94-4ba1-ab8a-eae27357cc7e.xlsx"  # the uploaded name you used earlier (kept as candidate)
]

def _find_columns(df):
    """
    Find likely code_col and name_col heuristically from DataFrame columns.
    """
    cols = list(df.columns)
    code_col = None
    name_col = None
    for c in cols:
        cu = c.upper()
        if ('BIC' in cu) or (('CODE' in cu) and ('BIC' in cu or 'SWIFT' in cu)):
            code_col = c
            break
    if not code_col:
        for c in cols:
            cu = c.upper()
            if 'CODE' in cu:
                code_col = c
                break

    for c in cols:
        cu = c.upper()
        if 'NOM' in cu or 'NAME' in cu or 'NOMS' in cu:
            name_col = c
            break
    if not name_col:
        candidate = None
        best_alpha = 0.0
        for c in cols:
            sample = ' '.join([str(x) for x in df[c].dropna().astype(str).head(20).tolist()])
            if not sample:
                continue
            alpha_frac = sum(ch.isalpha() for ch in sample) / max(1, len(sample))
            if alpha_frac > best_alpha:
                best_alpha = alpha_frac
                candidate = c
        name_col = candidate
    return code_col, name_col


def bundled_base_path() -> Path:
    """
    Retourne le chemin racine d'où lire les fichiers "embarqués".
    - Si l'app est packagée par PyInstaller (--onefile), les resources sont extraites
      temporairement dans sys._MEIPASS.
    - Sinon, retourne la racine du projet (deux niveaux au-dessus de ce fichier).
    """
    if getattr(sys, "frozen", False) and hasattr(sys, "_MEIPASS"):
        return Path(sys._MEIPASS)
    # adjust parents count depending on file location; this file is backend/app/extractor_manager.py
    # parents[2] points to repo root (pdf-extractor/)
    return Path(__file__).resolve().parents[2]

def _user_override_bic_paths() -> List[Path]:
    """
    Emplacements où un admin/utilisateur peut déposer un bic_codes.xlsx modifiable.
    Ordre de priorité (testé dans load_bic_mapping) :
      1) variable d'environnement PDF_SWIFT_DATA_DIR si définie
      2) dossier commun ProgramData (Windows) -> {PROGRAMDATA}/PDF_Swift_Extractor/data
      3) dossier local de l'utilisateur -> %LOCALAPPDATA%/PDF_Swift_Extractor/data ou ~/ .pdf_swift_extractor/data
    """
    paths = []
    env = os.getenv("PDF_SWIFT_DATA_DIR")
    if env:
        paths.append(Path(env))

    # Windows common appdata (ProgramData)
    programdata = os.getenv("PROGRAMDATA")
    if programdata:
        paths.append(Path(programdata) / "PDF_Swift_Extractor" / "data")

    # Windows local appdata or cross-platform user dir
    localappdata = os.getenv("LOCALAPPDATA") or os.getenv("XDG_DATA_HOME")
    if localappdata:
        paths.append(Path(localappdata) / "PDF_Swift_Extractor" / "data")

    # fallback to user home hidden dir
    paths.append(Path.home() / ".pdf_swift_extractor" / "data")

    return paths

def load_bic_mapping(xlsx_path: Optional[str] = None, sheet_name: Optional[str] = 0) -> Dict[str, str]:
    """
    Charge (et met en cache) la table BIC -> nom de banque depuis un fichier Excel.
    Logique améliorée pour permettre une mise à jour manuelle après installation :
      - si xlsx_path explicit fourni -> utilisé (existing behaviour)
      - sinon : on cherche d'abord dans des emplacements externes éditables (ProgramData, user dir,
        variable d'env PDF_SWIFT_DATA_DIR)
      - sinon : on cherche dans le bundle embarqué (bundled_base_path()/data)
      - sinon : on retombe sur les chemins _DEFAULT_XLS_PATHS comme avant
    """
    global _cached_mapping, _cached_mapping_path
    import pandas as pd  # lazy import

    # 1) if explicit path provided, prefer it
    if xlsx_path:
        p = Path(xlsx_path)
        if not p.exists():
            raise FileNotFoundError(f"Provided xlsx_path not found: {xlsx_path}")
    else:
        # 2) check user-writable override locations (ProgramData, LOCALAPPDATA, env var)
        p = None
        for base in _user_override_bic_paths():
            candidate = base / "bic_codes.xlsx"
            if candidate.exists():
                p = candidate
                break
            # also accept alternative names
            alt = base / "bic.xlsx"
            if alt.exists():
                p = alt
                break

        # 3) check bundled data dir (this will work for --onedir and for files added with --add-data)
        if p is None:
            bundled = bundled_base_path() / "data"
            for name in ("bic_codes.xlsx", "bic.xlsx"):
                cand = bundled / name
                if cand.exists():
                    p = cand
                    break

        # 4) fallback to original candidate list (relative to current working dir)
        if p is None:
            for cand in _DEFAULT_XLS_PATHS:
                if Path(cand).exists():
                    p = Path(cand)
                    break

        if p is None:
            raise FileNotFoundError(
                "Aucun fichier Excel trouvé. Place your Excel mapping in one of: "
                + ", ".join(_DEFAULT_XLS_PATHS)
                + " or a writable location like %PROGRAMDATA%\\PDF_Swift_Extractor\\data\\bic_codes.xlsx "
                + "or set environment variable PDF_SWIFT_DATA_DIR to a folder containing bic_codes.xlsx"
            )

    # cache check
    pstr = str(Path(p).resolve())
    if _cached_mapping is not None and _cached_mapping_path == pstr:
        return _cached_mapping

    df = pd.read_excel(pstr, sheet_name=sheet_name, dtype=str)
    df = df.dropna(axis=1, how='all')

    code_col, name_col = _find_columns(df)
    if not code_col:
        raise ValueError(f"Impossible de détecter la colonne code BIC dans {pstr}. Colonnes: {list(df.columns)}")
    if not name_col:
        logger.warning("load_bic_mapping: impossible de détecter colonne 'nom' ; les valeurs de nom seront vides.")

    mapping: Dict[str, str] = {}
    for _, row in df.iterrows():
        code_val = (str(row.get(code_col) or "")).strip()
        name_val = (str(row.get(name_col) or "")).strip() if name_col else ""
        if not code_val or code_val.lower() in ("nan", "none"):
            continue
        code_clean = re.sub(r'\s+', '', code_val).upper()
        key = code_clean[:8]
        if not key:
            continue
        mapping[key] = name_val

    _cached_mapping = mapping
    _cached_mapping_path = pstr
    logger.info("load_bic_mapping: loaded %d entries from %s", len(mapping), pstr)
    return mapping


FALLBACK_11_RE = re.compile(r'\b([A-Z0-9]{11})\b')

# remplace la fonction get_donneur_from_f52 existante par ceci
IDENTIFIER_RE_AFTER_LABEL = re.compile(
    r"(?i)(?:IdentifierCode|Identifier Code|Identifiercode|Code d'identifiant|Code d identifiant|IDENTIFIERCODE)\s*[:\-\s]*\n?\s*([A-Z0-9]{11})"
)

# words that look like labels and should NOT be accepted as code
_BAD_LABEL_TOKENS = {
    "IDENTIFIER", "IDENTIFIERC", "PARTYIDENTI", "PARTYIDENT", "IDENTIFIANT", "IDENTIFIERCODE",
    "PARTY", "PARTYIDENTIFIER"
}


# label regex (variantes FR/EN)
_LABEL_RE = re.compile(
    r"(?i)(?:IdentifierCode|Identifier Code|Identifiercode|Code d'identifiant|Code d identifiant|identifiant de partie|IDENTIFIERCODE)\s*[:\-\s]*",
    re.M
)

_BAD_LABEL_PREFIXES = ("IDENTIF", "PARTYIDENT", "PARTY", "IDENTIFIANT")

def _find_identifier_after_label(text: str, lookahead_chars: int = 600) -> Optional[str]:
    """
    Cherche après un label 'IdentifierCode' un token alphanumérique 6..11 caractères,
    autorise lignes vides entre label et token, privilégie tokens contenant des lettres.
    """
    if not text:
        return None
    txt = text.replace('\r', '\n')
    m = _LABEL_RE.search(txt)
    if not m:
        return None
    start = m.end()
    tail = txt[start: start + lookahead_chars]
    # find candidates 6..11 chars
    toks = re.findall(r'\b([A-Z0-9]{6,11})\b', tail, flags=re.I)
    toks = [t.upper() for t in toks]
    # filter label-like tokens
    toks = [t for t in toks if not any(t.startswith(pref) for pref in _BAD_LABEL_PREFIXES)]
    if not toks:
        return None
    # prefer token with a letter (likely BIC), otherwise return first
    for t in toks:
        if re.search(r'[A-Z]', t):
            return t
    return toks[0]

def get_donneur_from_f52(f52_text: Optional[str], message_text: Optional[str] = None, xlsx_path: Optional[str] = None) -> Optional[str]:
    """
    Robust extract:
     - try inside F52A block: find label then next token (6..11 chars), preferring alpha tokens
     - if not found, try search on the whole message (cross-page)
     - if still not found, attempt a last-resort token in F52A that contains letters
     - then map first 8 chars to bank name using load_bic_mapping (if available)
    Returns: "CODE11/BANK NAME" if mapping found, else CODE (or None).
    """
    # normalize and remove xml-like tags
    def _norm(s):
        return re.sub(r'<[^>]+>', ' ', (s or "")).replace('\r', '\n')

    f52 = _norm(f52_text)
    full = _norm(message_text) if message_text else None

    # 1) try strictly in F52A block
    code = _find_identifier_after_label(f52, lookahead_chars=800)
    # 2) if not found, try whole message (cross-page)
    if not code and full:
        code = _find_identifier_after_label(full, lookahead_chars=1200)
    # 3) last-resort: try to find first alnum token with letters in F52A
    if not code and f52:
        m = re.search(r'\b([A-Z][A-Z0-9]{5,10})\b', f52, flags=re.I)
        if m:
            tok = m.group(1).upper()
            if not any(tok.startswith(pref) for pref in _BAD_LABEL_PREFIXES):
                code = tok

    if not code:
        return None

    # attempt mapping to bank name (uses cached loader)
    try:
        mapping = load_bic_mapping(xlsx_path=xlsx_path)
    except Exception:
        mapping = {}

    key8 = code[:8].upper()
    bank = mapping.get(key8)
    if bank:
        bank_clean = re.sub(r'\s{2,}', ' ', bank).strip()
        return f"{code}/{bank_clean}"
    return code


# -----------------------------
# Dispatcher / workbook logic (existing)
# -----------------------------
def detect_message_type(text: str) -> Optional[str]:
    """
    Detect the MT type (e.g. "202", "103", "910") from extracted text.
    Returns the numeric string (e.g. "202") or None.
    """
    if not text:
        return None

    m = MT_DETECT.search(text)
    if m:
        mt = m.group(1)
        logger.debug("detect_message_type: primary MT_DETECT matched -> %s", mt)
        return mt

    m2 = IDENTIFIER_FIN_RE.search(text)
    if m2:
        mt = m2.group(1)
        logger.debug("detect_message_type: IDENTIFIER_FIN_RE matched -> %s", mt)
        return mt

    logger.debug("detect_message_type: no MT type matched")
    return None


def extract_dispatch(pdf_path: Path) -> List[Dict]:
    """
    Dispatcher intelligent :
      - si le PDF contient plusieurs messages -> utilise mt_multi.extract_messages_from_pdf
      - sinon -> utilise extract_single (retourne [row])
    Retourne toujours une LISTE de rows.
    """
    p = Path(pdf_path)
    # quick text extraction using existing helper
    text = ""
    try:
        text = extract_text_mt202(p)
    except Exception as e:
        logger.debug("extract_dispatch: extract_text_mt202 failed (%s), falling back to pdfplumber", e)
        try:
            import pdfplumber
            s = ""
            with pdfplumber.open(str(p)) as pdf:
                for page in pdf.pages[:2]:
                    s += "\n" + (page.extract_text() or "")
            text = s
        except Exception as e2:
            logger.warning("extract_dispatch: quick pdfplumber fallback failed for %s: %s", p.name, e2)
            text = ""

    # If multi-message extractor available, use its split logic to decide
    if HAS_MT_MULTI and mt_multi_module:
        try:
            blocks = mt_multi_module._split_messages(text)
            if blocks and len(blocks) > 1:
                logger.info("%s: detected %d messages (using mt_multi).", p.name, len(blocks))
                rows = mt_multi_module.extract_messages_from_pdf(p)
                # ensure backward compatibility: set institution_name from donneur_dordre if missing
                for r in rows:
                    if "institution_name" not in r or not r.get("institution_name"):
                        r["institution_name"] = r.get("donneur_dordre") or r.get("donneur d'ordre") or None
                    for k in ["code_banque", "date_reference", "reference", "type_MT", "pays_iso3", "beneficiaire", "montant", "devise", "source_pdf"]:
                        if k not in r:
                            r[k] = None
                return rows
        except Exception as e:
            logger.exception("extract_dispatch: mt_multi detection/extraction failed for %s: %s", p.name, e)
            # fall through to single extractor

    # fallback: treat as single message
    single_row = extract_single(p)
    return [single_row]


def _ensure_minimal_row(p: Path, mt_type: Optional[str] = None) -> Dict:
    """Return a minimal row template used when extraction not performed or failed."""
    return {
        "code_banque": None,
        "date_reference": None,
        "reference": None,
        "type_MT": f"fin.{mt_type}" if mt_type else None,
        "pays_iso3": None,
        "institution_name": None,
        "beneficiaire": None,
        "montant": None,
        "devise": None,
        "source_pdf": p.name
    }


def extract_single(pdf_path: Path) -> Dict:
    """
    Dispatch extraction for a single pdf_path (Path or str).
    Returns a dict with fields (internal keys). The create_workbook function maps
    'institution_name' -> "donneur d'ordre" when writing the summary sheet.
    """
    p = Path(pdf_path)
    if not p.exists():
        logger.error("extract_single: file not found: %s", p)
        return _ensure_minimal_row(p)

    # read text (use helper from mt202 for consistent behavior)
    try:
        text = extract_text_mt202(p)
    except Exception as e:
        logger.exception("extract_single: extract_text_mt202 failed for %s: %s", p.name, e)
        # fallback quick text extraction
        try:
            import pdfplumber
            s = ""
            with pdfplumber.open(str(p)) as pdf:
                for page in pdf.pages[:2]:
                    s += "\n" + (page.extract_text() or "")
            text = s
        except Exception as e2:
            logger.exception("extract_single: fallback pdfplumber failed for %s: %s", p.name, e2)
            return _ensure_minimal_row(p)

    mt = detect_message_type(text)
    if not mt:
        logger.info("%s: MT type not found in text", p.name)
        row = _ensure_minimal_row(p, mt_type=None)
        row["source_pdf"] = p.name
        return row

    extractor = EXTRACTOR_MAP.get(mt)
    if not extractor:
        logger.info("%s: type detected -> %s but no extractor implemented", p.name, mt)
        row = _ensure_minimal_row(p, mt_type=mt)
        row["source_pdf"] = p.name
        return row

    try:
        row = extractor(p)
        if not isinstance(row, dict):
            logger.error("%s: extractor returned non-dict result: %r", p.name, row)
            row = _ensure_minimal_row(p, mt_type=mt)
        else:
            required = ["code_banque", "date_reference", "reference", "type_MT", "pays_iso3",
                        "institution_name", "beneficiaire", "montant", "devise", "source_pdf"]
            for k in required:
                if k not in row:
                    row[k] = None
            if not row.get("institution_name") and row.get("donneur_dordre"):
                row["institution_name"] = row.get("donneur_dordre")
            if not row.get("type_MT"):
                row["type_MT"] = f"fin.{mt}"
            if not row.get("source_pdf"):
                row["source_pdf"] = p.name
        logger.info("%s: extracted via MT%s", p.name, mt)
        return row
    except Exception as e:
        logger.exception("Extraction failed for %s (MT%s): %s", p.name, mt, e)
        row = _ensure_minimal_row(p, mt_type=mt)
        row["error"] = str(e)
        return row


def _sanitize_sheet_title(name: str, max_len: int = 31) -> str:
    """Make a safe Excel sheet name (no invalid chars, limited length)."""
    if not name:
        name = "sheet"
    sanitized = re.sub(r'[:\\\/\?\*\[\]]+', '_', name)
    sanitized = sanitized.strip()
    if len(sanitized) > max_len:
        sanitized = sanitized[:max_len]
    if not sanitized:
        sanitized = "sheet"
    return sanitized


def create_workbook(rows: List[Dict], out_dir: Path) -> Path:
    """
    Create an Excel workbook with:
      - a 'summary' sheet containing one row per extracted file (display headers in French)
      - one additional sheet per file with key/value pairs (debug-friendly)
    Returns the Path to the saved workbook.
    """
    out_dir = Path(out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    out_path = out_dir / f"swift_extraction_{ts}.xlsx"

    wb = Workbook()
    summary = wb.active
    summary.title = "summary"

    # summary headers (user-facing)
    display_headers = [
        "code_banque",
        "date_reference",
        "reference",
        "type_MT",
        "pays_iso3",
        "donneur d'ordre",
        "Bénéficiaire",
        "montant",
        "devise",
        "source_pdf"
    ]
    summary.append(display_headers)

    # write summary rows (map internal keys -> display)
    for r in rows:
        # prefer institution_name, else new key donneur_dordre
        donneur = r.get("institution_name") or r.get("donneur_dordre") or r.get("donneur d'ordre") or None
        beneficiaire = r.get("beneficiaire") or None
        summary.append([
            r.get("code_banque"),
            r.get("date_reference"),
            r.get("reference"),
            r.get("type_MT"),
            r.get("pays_iso3"),
            donneur,
            beneficiaire,
            r.get("montant"),
            r.get("devise"),
            r.get("source_pdf")
        ])

    # create per-file sheets (key/value)
    used_names = set()
    for r in rows:
        base = r.get("source_pdf", "sheet")
        title = _sanitize_sheet_title(str(base))
        original = title
        i = 1
        while title in used_names or title in wb.sheetnames:
            suffix = f"_{i}"
            max_base_len = 31 - len(suffix)
            title = (original[:max_base_len] + suffix) if len(original) > max_base_len else (original + suffix)
            i += 1
        used_names.add(title)
        ws = wb.create_sheet(title=title)

        ordered_keys = [
            "code_banque", "date_reference", "reference", "type_MT", "pays_iso3",
            "institution_name", "beneficiaire", "montant", "devise", "source_pdf"
        ]
        written = set()
        for k in ordered_keys:
            if k in r:
                label = "donneur d'ordre" if k == "institution_name" else ("Bénéficiaire" if k == "beneficiaire" else k)
                ws.append([label, r.get(k)])
                written.add(k)
        for k, v in r.items():
            if k in written:
                continue
            label = "donneur d'ordre" if k == "institution_name" else ("Bénéficiaire" if k == "beneficiaire" else k)
            ws.append([label, v])

        # adjust column widths heuristically
        try:
            max_len_col1 = max((len(str(row[0])) for row in ws.values if row[0] is not None), default=10)
            max_len_col2 = max((len(str(row[1])) for row in ws.values if len(row) > 1 and row[1] is not None), default=10)
            ws.column_dimensions[get_column_letter(1)].width = min(60, max(12, max_len_col1 + 2))
            ws.column_dimensions[get_column_letter(2)].width = min(80, max(12, max_len_col2 + 8))
        except Exception:
            pass

    wb.save(out_path)
    logger.info("Workbook created: %s", out_path)
    return out_path
=== ./dist/launcher/_internal/backend/app/api.py ===
# backend/app/api.py
from fastapi import APIRouter, Depends, UploadFile, File, HTTPException, status, Form
from fastapi.responses import FileResponse, JSONResponse
from fastapi.security import OAuth2PasswordRequestForm, OAuth2PasswordBearer
from jose import jwt, JWTError
from datetime import timedelta, datetime
from pathlib import Path
import shutil, os
from .db import SessionLocal, init_db, get_user, verify_password
from .db import create_user as create_user_db
from .utils import logger
from .extractor_manager import extract_single, create_workbook
from typing import List

router = APIRouter()
BASE = Path(__file__).resolve().parent.parent
DATA_DIR = BASE / "data"
RAW_DIR = BASE.parent / "data" / "raw"
OUT_DIR = BASE.parent / "output" / "tables"
LOG_FILE = BASE.parent.parent / "logs" / "app.log"

# auth / secrets
SECRET_KEY = os.environ.get("JWT_SECRET", "dev_secret_change_me")
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 60

oauth2_scheme = OAuth2PasswordBearer(tokenUrl="token")

def create_access_token(data: dict, expires_delta: timedelta = None):
    to_encode = data.copy()
    if expires_delta:
        expire = datetime.utcnow() + expires_delta
    else:
        expire = datetime.utcnow() + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    to_encode.update({"exp": expire})
    return jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)

def get_current_user(token: str = Depends(oauth2_scheme)):
    credentials_exception = HTTPException(status_code=401, detail="Could not validate credentials")
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        username: str = payload.get("sub")
        role: str = payload.get("role")
        if username is None:
            raise credentials_exception
    except JWTError:
        raise credentials_exception
    db = SessionLocal()
    user = db.query(__import__("app.db", fromlist=["User"]).User).filter_by(username=username).first()
    db.close()
    if not user:
        raise credentials_exception
    return {"username": username, "role": role}

def require_role(min_role: str):
    order = {"user":0, "admin":1, "superadmin":2}
    def dep(user = Depends(get_current_user)):
        if order.get(user["role"], 0) < order.get(min_role, 0):
            raise HTTPException(status_code=403, detail="Insufficient privileges")
        return user
    return dep

@router.post("/token")
def login(form_data: OAuth2PasswordRequestForm = Depends()):
    db = SessionLocal()
    try:
        u = db.query(__import__("app.db", fromlist=["User"]).User).filter_by(username=form_data.username).first()
        if not u or not verify_password(form_data.password, u.hashed_password):
            raise HTTPException(status_code=400, detail="Incorrect username or password")
        token = create_access_token({"sub": u.username, "role": u.role})
        return {"access_token": token, "token_type": "bearer"}
    finally:
        db.close()

@router.post("/upload")
async def upload(files: List[UploadFile] = File(...), current = Depends(require_role("user"))):
    RAW_DIR.mkdir(parents=True, exist_ok=True)
    OUT_DIR.mkdir(parents=True, exist_ok=True)
    rows = []
    for up in files:
        dest = RAW_DIR / up.filename
        with open(dest, "wb") as f:
            shutil.copyfileobj(up.file, f)
        logger.info(f"Saved upload {up.filename}")
        r = extract_single(dest)
        rows.append(r)
    out_file = create_workbook(rows, OUT_DIR)
    return FileResponse(str(out_file), media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet", filename=out_file.name)

@router.get("/runs")
def list_runs(current = Depends(require_role("admin"))):
    OUT_DIR.mkdir(parents=True, exist_ok=True)
    files = sorted([p.name for p in OUT_DIR.glob("*.xlsx")], reverse=True)
    return {"runs": files}

@router.get("/logs")
def get_logs(skip: int = 0, limit: int = 200, current = Depends(require_role("admin"))):
    if not LOG_FILE.exists():
        return {"lines": []}
    with open(LOG_FILE, "r", encoding="utf-8") as f:
        lines = f.readlines()
    # return last lines
    lines = lines[-(skip + limit):] if (skip+limit) <= len(lines) else lines
    return {"lines": [l.rstrip("\n") for l in lines]}

# bootstrap helper route (only for dev)
@router.post("/bootstrap_users")
def bootstrap_users(current = Depends(require_role("superadmin"))):
    init_db()
    db = SessionLocal()
    # create a default set if not existing
    for u,p,r in [("alice","alicepass","user"),("bob","bobpass","admin"),("root","rootpass","superadmin")]:
        existing = db.query(__import__("app.db", fromlist=["User"]).User).filter_by(username=u).first()
        if not existing:
            create_user_db(db, u, p, r)
    db.close()
    return {"ok": True}
=== ./dist/launcher/_internal/backend/app/create_users.py ===
# backend/app/create_users.py
#!/usr/bin/env python3
import argparse, sys
from pathlib import Path
HERE = Path(__file__).resolve().parent
sys.path.insert(0, str(HERE))

from db import init_db, SessionLocal, create_user

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("username")
    parser.add_argument("password")
    parser.add_argument("--role", default="user", choices=["user", "admin", "superadmin"])
    args = parser.parse_args()

    init_db()
    db = SessionLocal()
    try:
        u = create_user(db, args.username, args.password, role=args.role)
        print(f"Created user: {u.username} (role={u.role})")
    finally:
        db.close()

if __name__ == "__main__":
    main()
=== ./dist/launcher/_internal/backend/app/utils.py ===
# backend/app/utils.py
import logging
from pathlib import Path

LOG_DIR = Path(__file__).resolve().parent.parent.parent / "logs"
LOG_DIR.mkdir(parents=True, exist_ok=True)
LOG_FILE = LOG_DIR / "app.log"

def setup_logger():
    logger = logging.getLogger("pdf_extractor")
    if logger.handlers:
        return logger
    logger.setLevel(logging.INFO)
    fh = logging.FileHandler(LOG_FILE, encoding="utf-8")
    formatter = logging.Formatter("%(asctime)s [%(levelname)s] %(message)s")
    fh.setFormatter(formatter)
    logger.addHandler(fh)
    # also console handler
    ch = logging.StreamHandler()
    ch.setFormatter(formatter)
    logger.addHandler(ch)
    return logger

logger = setup_logger()
=== ./dist/launcher/_internal/backend/app/extractors/bic_utils.py ===
# backend/app/extractors/bic_utils.py
"""
Utilities to extract the IdentifierCode from F52A and to map the first-8-chars
to a human-readable bank name using an Excel table (data/bic_codes.xlsx).

Provides:
 - load_bic_mapping(xlsx_path: Optional[str]) -> Dict[str, str]
 - map_code_to_name(code: str, xlsx_path: Optional[str]) -> Optional[str]
 - get_name_for_code(code: str, xlsx_path: Optional[str]) -> Optional[str]  # alias for compatibility
 - get_donneur_from_f52(f52_text, message_text=None, xlsx_path=None) -> Optional[str]
"""
from pathlib import Path
import re
from functools import lru_cache
from typing import Optional, Dict

try:
    import pandas as pd
except Exception as e:
    raise RuntimeError("pandas is required by bic_utils.py (pip install pandas)") from e

# exact label to find (case-insensitive)
_LABEL_TEXT = "IdentifierCode: Code d'identifiant:"
_LABEL_RE = re.compile(re.escape(_LABEL_TEXT), re.I)

# strict code rule: only uppercase letters, length 8..11
_STRICT_TOKEN_RE = re.compile(r'^[A-Z]{8,11}$')

# fallback token pattern (alphanumeric 8..11) - used only if strict not found
_FALLBACK_TOKEN_RE = re.compile(r'\b([A-Z0-9]{8,11})\b', re.I)

# module-level cache
_BIC_MAP_CACHE: Optional[Dict[str, str]] = None
_BIC_FULLKEY_MAP: Optional[Dict[str, str]] = None


def _find_strict_identifier_in_f52(f52_text: str) -> Optional[str]:
    """
    Apply the strict rule:
      - find exact label (case-insensitive) inside f52_text
      - inspect: same line after label, next line, next-next line
      - return the first token matching ^[A-Z]{8,11}$
    """
    if not f52_text:
        return None

    lines = [ln.rstrip() for ln in f52_text.splitlines()]
    label_idx = None
    label_span_end = None
    for i, ln in enumerate(lines):
        m = _LABEL_RE.search(ln)
        if m:
            label_idx = i
            label_span_end = m.end()
            break

    if label_idx is None:
        return None

    candidates = []

    # same line after label
    same_line_after = lines[label_idx][label_span_end:].strip()
    if same_line_after:
        candidates.append(same_line_after)

    # next two lines (allow blank lines)
    for j in (label_idx + 1, label_idx + 2):
        if j < len(lines):
            candidates.append(lines[j].strip())

    # evaluate candidates strictly: only A-Z length 8..11
    for cand in candidates:
        if not cand:
            continue
        # split candidate into tokens of letters only (strip punctuation)
        toks = re.findall(r'[A-Z]+', cand.upper())
        for t in toks:
            if _STRICT_TOKEN_RE.match(t):
                return t
    return None


@lru_cache(maxsize=1)
def load_bic_mapping(xlsx_path: Optional[str] = None) -> Dict[str, str]:
    """
    Load the BIC mapping Excel file and return a dict mapping 8-char key -> bank name.
    Also populate a full-key map for 11-char exact matches.
    Default path: data/bic_codes.xlsx

    Expected columns: try to detect columns for code and name (flexible).
    """
    global _BIC_MAP_CACHE, _BIC_FULLKEY_MAP
    if _BIC_MAP_CACHE is not None and _BIC_FULLKEY_MAP is not None:
        return _BIC_MAP_CACHE

    fp = Path(xlsx_path) if xlsx_path else Path("data/bic_codes.xlsx")
    mapping: Dict[str, str] = {}
    mapping_full: Dict[str, str] = {}

    if not fp.exists():
        _BIC_MAP_CACHE = {}
        _BIC_FULLKEY_MAP = {}
        return _BIC_MAP_CACHE

    df = pd.read_excel(fp, dtype=str)
    # normalize column names
    cols = [c for c in df.columns]

    # heuristics to find code and name columns
    code_col = None
    name_col = None
    col_upper = [c.strip().upper() for c in cols]

    # common name candidates
    for i, cu in enumerate(col_upper):
        if cu in ("CODE", "BIC", "CODE BIC", "BIC_CODE", "BIC8", "CODE8", "CODE_BIC", "CODEBIC"):
            code_col = cols[i]
            break
    if not code_col:
        # fallback: pick first column whose values look like BICs/8..11 alnum
        for c in cols:
            sample = df[c].dropna().astype(str)
            if not sample.empty and sample.str.match(r'^[A-Z0-9]{6,12}$', case=False).sum() > 0:
                code_col = c
                break

    for i, cu in enumerate(col_upper):
        if cu in ("NOMS", "NOM", "NAME", "BANK", "INSTITUTION", "NOMINSTITUTION"):
            name_col = cols[i]
            break
    if not name_col:
        # fallback: choose the first non-code column
        for c in cols:
            if c != code_col:
                name_col = c
                break

    if not code_col:
        _BIC_MAP_CACHE = {}
        _BIC_FULLKEY_MAP = {}
        return _BIC_MAP_CACHE

    # build mapping
    for _, row in df.iterrows():
        raw_code = str(row.get(code_col) or "").strip()
        if not raw_code:
            continue
        raw_code = raw_code.replace(" ", "").upper()
        raw_name = ""
        if name_col:
            raw_name = str(row.get(name_col) or "").strip()
        # map first 8 chars -> name
        key8 = raw_code[:8]
        if key8:
            if raw_name:
                mapping[key8] = raw_name
            else:
                # if no name column found, use the raw_code as fallback value (rare)
                mapping.setdefault(key8, raw_code)
        # also keep full mapping for exact 11-char keys (useful for BEACCMCX100)
        if len(raw_code) >= 8:
            mapping_full[raw_code] = raw_name or mapping.get(raw_code[:8], "")

    _BIC_MAP_CACHE = mapping
    _BIC_FULLKEY_MAP = mapping_full
    return _BIC_MAP_CACHE


def map_code_to_name(code: str, xlsx_path: Optional[str] = None) -> Optional[str]:
    """
    Map a raw code (8..11 chars) to bank name using loaded mapping.
    Rules:
      - If code exactly matches an 11-char full key present in the sheet (e.g. BEACCMCX100),
        then prefer that exact mapping.
      - Otherwise map using the first 8 characters.
    Returns the bank name or None.
    """
    if not code:
        return None
    _ = load_bic_mapping(xlsx_path=xlsx_path)  # populate caches
    global _BIC_MAP_CACHE, _BIC_FULLKEY_MAP
    code_u = code.strip().upper()
    # prefer exact full key if present
    if _BIC_FULLKEY_MAP and code_u in _BIC_FULLKEY_MAP and _BIC_FULLKEY_MAP[code_u]:
        return _BIC_FULLKEY_MAP[code_u]
    key8 = code_u[:8]
    if _BIC_MAP_CACHE and key8 in _BIC_MAP_CACHE:
        return _BIC_MAP_CACHE[key8]
    return None


# Backwards-compatibility alias expected by older code
def get_name_for_code(code: str, xlsx_path: Optional[str] = None) -> Optional[str]:
    """
    Compatibility wrapper used by some older extractors.
    Returns same as map_code_to_name.
    """
    return map_code_to_name(code, xlsx_path=xlsx_path)


def get_donneur_from_f52(f52_text: Optional[str], message_text: Optional[str] = None, xlsx_path: Optional[str] = None) -> Optional[str]:
    """
    Public helper used by extractors:
     - find strict IdentifierCode token inside f52_text (or within message_text if absent)
     - map to bank name (first 8 chars) and return "CODE/Bank Name"
     - if no mapping, return CODE (the extracted token)
     - return None if no token found

    NOTE: token is expected to be letters A-Z only (8..11) based on your rule.
    """
    code = None
    if f52_text:
        code = _find_strict_identifier_in_f52(f52_text)
    if not code and message_text:
        # try in the full message (cross-page)
        code = _find_strict_identifier_in_f52(message_text)
    # final fallback: search for an 8..11 alpha token inside f52 block
    if not code and f52_text:
        m = _FALLBACK_TOKEN_RE.search(f52_text)
        if m:
            cand = m.group(1).upper()
            # accept only all-letters candidate (user requested only letters as valid code)
            if re.fullmatch(r'[A-Z]{8,11}', cand):
                code = cand

    if not code:
        return None

    name = map_code_to_name(code, xlsx_path=xlsx_path)
    if name:
        return f"{code}/{name}"
    return code
=== ./dist/launcher/_internal/backend/app/extractors/mt103.py ===
# backend/app/extractors/mt103.py
"""
Extracteur MT103 — wrapper/variant spécifique.
Uses bic_utils.get_donneur_from_f52 for donor mapping.
"""

import re
from pathlib import Path
from typing import Optional
import pdfplumber

from backend.app.extractors.mt202 import (
    get_field_block,
    parse_amount,
    parse_date_YYMMDD,
    detect_country_from_text,
    extract_receiver_bic,
    parse_reference as parse_reference_mt202,
)
from backend.app.extractors.bic_utils import get_donneur_from_f52  # NEW

def parse_f32a_103(text: str) -> dict:
    blk = get_field_block(text, 'F32A') or text
    blk_clean = re.sub(r'#.*?#', '', blk, flags=re.S)
    result = {'date_reference': None, 'devise': None, 'montant': None}
    m_date = re.search(r'(?i)\bDate[:\s]*([0-9]{6})\b', blk_clean)
    if m_date:
        result['date_reference'] = parse_date_YYMMDD(m_date.group(1))
    else:
        m_date2 = re.search(r'(\d{6})', blk_clean)
        if m_date2:
            result['date_reference'] = parse_date_YYMMDD(m_date2.group(1))
    m_cur = re.search(r'(?i)\bDevise[:\s]*([A-Z]{3})\b', blk_clean)
    if m_cur:
        result['devise'] = m_cur.group(1)
    else:
        m_cur2 = re.search(r'(?i)Currency[:\s\S]{0,80}?([A-Z]{3})\b', blk_clean)
        if m_cur2:
            result['devise'] = m_cur2.group(1)
        else:
            m_cur3 = re.search(r'\b([A-Z]{3})\b', blk_clean)
            if m_cur3:
                result['devise'] = m_cur3.group(1)
    candidate = None
    m_line = re.search(r'(?im)^\s*(?:Montant|Amount)\s*[:\-]\s*(.*)$', blk_clean, flags=re.M)
    if m_line:
        line = m_line.group(1).strip()
        nums = re.findall(r'([0-9]+(?:[.,\s][0-9]{1,3})*(?:[.,][0-9]{1,2})?)', line)
        if nums:
            def digits_len(s): return len(re.sub(r'[^0-9]', '', s))
            candidate = max(nums, key=digits_len)
    if not candidate:
        nums_all = re.findall(r'([0-9]+(?:[.,\s][0-9]{1,3})*(?:[.,][0-9]{1,2})?)', blk_clean)
        if nums_all:
            def digits_len(s): return len(re.sub(r'[^0-9]', '', s))
            candidate = max(nums_all, key=digits_len)
    if candidate:
        result['montant'] = parse_amount(candidate)
    return result

def parse_f59_account(text: str) -> Optional[str]:
    blk = get_field_block(text, 'F59') or get_field_block(text, 'F59:')
    if not blk:
        return None
    blk_clean = re.sub(r'#.*?#', '', blk, flags=re.S)
    m = re.search(r'(?m)^\s*\/?([A-Z]{2}[0-9A-Z]{8,34})\b', blk_clean)
    if not m:
        m = re.search(r'\/([A-Z]{2}[0-9A-Z]{8,34})', blk_clean)
    if not m:
        m = re.search(r'([A-Z]{2}[0-9A-Z]{8,34})', blk_clean)
    if not m:
        return None
    candidate = m.group(1)
    candidate_norm = re.sub(r'\s+', '', candidate).upper()
    return candidate_norm

def parse_f52a_or_f50f_institution(text: str) -> Optional[str]:
    """
    Prefer F52A (donor) processed by bic_utils.get_donneur_from_f52.
    If absent, fallback to previous heuristics (F50F/F50).
    """
    # try F52A using strict bic_utils
    f52 = get_field_block(text, 'F52A')
    # If get_donneur_from_f52 returns code/name, use it
    donneur = None
    if f52:
        donneur = get_donneur_from_f52(f52, message_text=text)
        if donneur:
            return donneur

    # fallback: try to get a human-friendly name from F52A (previous logic)
    if f52:
        lines = [l.strip() for l in re.sub(r'<[^>]+>', ' ', f52).splitlines() if l.strip()]
        name_lines = []
        for ln in lines:
            up = ln.upper()
            if up.startswith("IDENTIFIER") or up.startswith("IDENTIFIERCODE") or up.startswith("CODE") or up.startswith("PARTYIDENTIFIER") or up.startswith("IDENTIFIANT"):
                continue
            if re.match(r'^\/[A-Z0-9\/\-]+', ln):
                continue
            if re.fullmatch(r'[A-Z0-9]{6,11}', ln.replace(' ', '')):
                continue
            if len(ln) > 1:
                name_lines.append(ln)
        if name_lines:
            for i, ln in enumerate(name_lines):
                up = ln.upper()
                if 'BANK' in up or 'BANQUE' in up or 'ORABANK' in up:
                    out = ln
                    if i+1 < len(name_lines) and len(name_lines[i+1]) < 40:
                        out = f"{out} / {name_lines[i+1]}"
                    return out.strip()
            out = ' '.join(name_lines[:2]).strip()
            return out

    # fallback to F50F / F50 (client giver)
    blk50 = get_field_block(text, 'F50F') or get_field_block(text, 'F50')
    if blk50:
        lines = [l.strip() for l in blk50.splitlines() if l.strip()]
        name_candidates = []
        for ln in lines:
            up = ln.upper()
            if up.startswith("NAMEANDADDRESS") or up.startswith("DETAILS") or re.search(r'[A-Za-z]', ln):
                if up.startswith("NUMBER") or up.startswith("PARTYIDENTIFIER") or up.startswith("COMPTE"):
                    continue
                if len(ln) >= 4 and re.search(r'[A-Za-z]', ln):
                    name_candidates.append(ln)
        if name_candidates:
            out = ' '.join(name_candidates[:2]).strip()
            return out

    return None

def extract_from_text(text: str, source: str = None) -> dict:
    row = {
        "type_MT": None,
        "code_banque": None,
        "sender_bic": None,
        "receiver_bic": None,
        "reference": None,
        "date_reference": None,
        "devise": None,
        "montant": None,
        "donneur_dordre": None,
        "beneficiaire": None,
        "pays_iso3": None,
        "source_pdf": source
    }
    m_type = re.search(r'\b(?:MT|FIN)[\s\-_]*(\d{3})\b', text, re.I)
    if m_type:
        row["type_MT"] = f"fin.{m_type.group(1)}".lower()
    else:
        row["type_MT"] = "fin.103"
    rb = extract_receiver_bic(text)
    row["code_banque"] = rb
    row["receiver_bic"] = rb
    try:
        ref = parse_reference_mt202(text)
        row["reference"] = ref
    except Exception:
        blk20 = get_field_block(text, 'F20')
        if blk20:
            for ln in blk20.splitlines():
                ln = ln.strip()
                if not ln:
                    continue
                if re.search(r'\d+\/\d+|\w+\/\w+|\d{2,}', ln):
                    row["reference"] = ln
                    break
            if not row["reference"]:
                row["reference"] = blk20.splitlines()[0].strip()
    f32 = parse_f32a_103(text)
    row["date_reference"] = f32.get("date_reference")
    row["devise"] = f32.get("devise")
    row["montant"] = f32.get("montant")

    # F52A or fallback F50F
    inst = parse_f52a_or_f50f_institution(text)
    row["donneur_dordre"] = inst

    # bénéficiaire
    row["beneficiaire"] = parse_f59_account(text)

    # country detection
    row["pays_iso3"] = detect_country_from_text(text)
    return row

def extract_block(block_text: str, source: str = None) -> dict:
    return extract_from_text(block_text, source=source)

def extract_for_mt103(pdf_path):
    txt = ""
    with pdfplumber.open(str(pdf_path)) as pdf:
        for page in pdf.pages:
            txt += "\n" + (page.extract_text() or "")
    return extract_from_text(txt, source=getattr(pdf_path, "name", str(pdf_path)))

if __name__ == "__main__":
    import sys
    from pprint import pprint
    if len(sys.argv) < 2:
        print("Usage: python mt103.py path/to/103.pdf")
        raise SystemExit(1)
    pprint(extract_for_mt103(sys.argv[1]))
=== ./dist/launcher/_internal/backend/app/extractors/mt202.py ===
# backend/app/extractors/mt202.py
"""
Extracteur MT202 (text-level). Fournit :
- helpers utilitaires (get_field_block, parse_amount, parse_date_YYMMDD, etc.)
- parse_f32a, extract_transaction_reference (robuste)
- expose aussi parse_reference pour compatibilité avec mt103
- utilise bic_utils.get_donneur_from_f52 pour la valeur donneur_dordre (CODE/NAME)
"""

import re
from datetime import datetime
from typing import Optional
from dateutil import parser as dateparser
import pdfplumber

# bic helper (may return "CODE/Name" or "CODE")
try:
    from backend.app.extractors.bic_utils import get_donneur_from_f52, map_code_to_name
except Exception:
    # fallback: define no-op functions if bic_utils missing
    def get_donneur_from_f52(*a, **k):
        return None
    def map_code_to_name(*a, **k):
        return None

# regex / constants
BIC_RE = re.compile(r'\b[A-Z]{4}[A-Z]{2}[A-Z0-9]{2}(?:[A-Z0-9]{3})?\b')
MT_RE = re.compile(r'\b(?:MT|FIN)[\s\-_]*(\d{3})\b', re.I)
CEMAC_MAP = {
    "CM": "CMR", "CMR": "CMR", "CAMEROON": "CMR",
    "GA": "GAB", "GAB": "GAB", "GABON": "GAB",
    "TD": "TCD", "TCD": "TCD", "CHAD": "TCD",
    "CG": "COG", "COG": "COG", "CONGO": "COG",
    "GQ": "GNQ", "GNQ": "GNQ", "EQUATORIAL GUINEA": "GNQ",
    "CF": "CAF", "CAF": "CAF", "CENTRAL AFRICAN REPUBLIC": "CAF"
}

# ---------- PDF text extractor ----------
def extract_text_from_pdf(path):
    txt = ""
    with pdfplumber.open(str(path)) as pdf:
        for page in pdf.pages:
            txt += "\n" + (page.extract_text() or "")
    # minor normalization
    txt = re.sub(r'(?mi)^\s*page\s+\d+\s*(?:of\s*\d+)?\s*$', '', txt, flags=re.M)
    txt = re.sub(r'\r', '\n', txt)
    # collapse excessive blank lines but keep paragraph separation
    txt = re.sub(r'\n{3,}', '\n\n', txt)
    return txt

# ---------- low-level helpers ----------
def get_field_block(text: str, field_label: str) -> Optional[str]:
    """
    Return the multiline text belonging to a tag Fxx (e.g. 'F52A' or 'F20') inside `text`.
    """
    if not text:
        return None
    # Try label with optional trailing colon/description and capture following lines until next Fxx or end
    pattern = re.compile(r'(?si)(' + re.escape(field_label) + r'[:\s]*)(.*?)(?=\nF\d{2}[A-Z]?:|\nF\d{2}\b|$)')
    m = pattern.search(text)
    return m.group(2).strip() if m else None

def parse_amount(s: Optional[str]) -> Optional[float]:
    if not s:
        return None
    s = s.strip()
    # keep digits, thousand separators, decimal separators, minus
    s = re.sub(r'[^\d,.\-\s]', '', s)
    s = s.replace('\xa0', ' ')
    # normalize: detect whether comma is decimal or dot is decimal
    if s.count(',') and s.count('.'):
        # decide by last separator position
        if s.rfind(',') > s.rfind('.'):
            # comma decimal -> remove dots (thousand), replace comma with dot
            s = s.replace('.', '').replace(',', '.')
        else:
            # dot decimal -> remove commas
            s = s.replace(',', '')
    else:
        if s.count(','):
            # comma may be decimal if last group length 1-2 digits
            idx = s.rfind(',')
            if len(s) - idx - 1 in (1, 2):
                s = s.replace('.', '').replace(',', '.')
            else:
                s = s.replace(',', '')
        else:
            # no comma, remove spaces
            s = s.replace(' ', '')
    try:
        return float(s)
    except Exception:
        return None

def parse_date_YYMMDD(s: Optional[str]) -> Optional[str]:
    if not s:
        return None
    s = s.strip()
    if re.fullmatch(r'\d{6}', s):
        yy = int(s[:2]); mm = int(s[2:4]); dd = int(s[4:6])
        year = 2000 + yy
        try:
            return datetime(year, mm, dd).date().isoformat()
        except Exception:
            return None
    try:
        d = dateparser.parse(s, dayfirst=False)
        return d.date().isoformat() if d else None
    except Exception:
        return None

def detect_country_from_text(txt: str) -> Optional[str]:
    if not txt:
        return None
    txtu = txt.upper()
    # try last token of lines
    for line in txtu.splitlines():
        parts = line.strip().split()
        if not parts: continue
        last = parts[-1].strip().strip(',').strip('.')
        if last in CEMAC_MAP:
            return CEMAC_MAP[last]
    # look for longer country name tokens
    for key in CEMAC_MAP:
        if len(key) > 2 and key in txtu:
            return CEMAC_MAP[key]
    # two-letter tokens
    tokens = re.findall(r'\b[A-Z]{2}\b', txtu)
    for t in tokens:
        if t in CEMAC_MAP:
            return CEMAC_MAP[t]
    return None

# ---------- helpers for reference robustness ----------
def _looks_like_amount(s: Optional[str]) -> bool:
    if not s:
        return False
    s_low = s.lower()
    if 'amount' in s_low or 'currency' in s_low or 'montant' in s_low:
        return True
    # detect numbers with thousand separators and decimal comma/dot
    if re.search(r'\b\d{1,3}(?:[.\s]\d{3})*(?:[.,]\d{1,2})\b', s):
        return True
    # detect patterns like "191.700,64" or "191700,64"
    if re.search(r'\d+[.,]\d{2}', s):
        return True
    return False

def extract_transaction_reference(full_text: str, block4_text: Optional[str]) -> Optional[str]:
    """
    Robust extraction of transaction reference.
    Priority:
      1) F20 / :20: inside block4 (handles value on next non-empty line)
      2) header 'Transaction Reference: <TOKEN>' (token = [A-Z0-9_-]{3,})
      3) safe fallback: small token search but avoid picking amounts
    Returns uppercase reference or None.
    """
    # 1) try block4 / F20
    b = block4_text or ""
    if b:
        # same-line pattern: "F20: S065..." or ":20:S065..."
        m = re.search(r'(?mi)^(?:\:20\:|F20[:\s]*)(.*)$', b, flags=re.M)
        if m:
            cand = m.group(1).strip()
            if not cand:
                # find next non-empty line after the matched line
                lines = b.splitlines()
                for i, ln in enumerate(lines):
                    if re.match(r'(?mi)^(?:\:20\:|F20[:\s]*)', ln):
                        j = i + 1
                        while j < len(lines) and not lines[j].strip():
                            j += 1
                        if j < len(lines):
                            cand = lines[j].strip()
                        break
            if cand and not _looks_like_amount(cand):
                tok = re.search(r'([A-Z0-9\-\_]{3,})', cand, flags=re.I)
                if tok:
                    return tok.group(1).upper()
        else:
            # handle label on its own line and value on next line:
            lines = b.splitlines()
            for i, ln in enumerate(lines):
                if re.match(r'(?mi)^\s*(?:F20[:\s]*|:20:)', ln):
                    # see if same-line value
                    same = re.sub(r'(?mi)^\s*(?:F20[:\s]*|:20:)\s*', '', ln).strip()
                    if same:
                        cand = same
                    else:
                        j = i + 1
                        while j < len(lines) and not lines[j].strip():
                            j += 1
                        cand = lines[j].strip() if j < len(lines) else ""
                    if cand and not _looks_like_amount(cand):
                        tok = re.search(r'([A-Z0-9\-\_]{3,})', cand, flags=re.I)
                        if tok:
                            return tok.group(1).upper()
                    break

    # 2) header "Transaction Reference: TOKEN"
    m2 = re.search(r'(?mi)Transaction\s+Reference\s*[:\s]*([A-Z0-9\-\_]{3,})', full_text)
    if m2:
        cand = m2.group(1).strip()
        if not _looks_like_amount(cand):
            return cand.upper()

    # 3) safe fallback: look for a line immediately after "F20" label anywhere in full_text
    m_label = re.search(r'(?mi)(?:F20[:\s]*|:20:)\s*$', full_text, flags=re.M)
    if m_label:
        # find the position, then take next non-empty line
        pos = m_label.end()
        tail = full_text[pos: pos + 400]
        lines = tail.splitlines()
        for ln in lines:
            ln = ln.strip()
            if not ln:
                continue
            if not _looks_like_amount(ln):
                tok = re.search(r'([A-Z0-9\-\_]{3,})', ln, flags=re.I)
                if tok:
                    return tok.group(1).upper()
            break

    # nothing reliable
    return None

# provide parse_reference wrapper for backwards compatibility
def parse_reference(text: str) -> Optional[str]:
    """
    Backwards-compatible wrapper expected by mt103: compute an appropriate block4
    (prefer F20 or Block 4) and call the robust extractor.
    """
    if not text:
        return None
    block4 = get_field_block(text, 'F20') or get_field_block(text, ':20') or get_field_block(text, 'Block 4') or get_field_block(text, 'Block4') or text
    return extract_transaction_reference(text, block4)

# ---------- field parsers ----------
def parse_f32a(text: str) -> dict:
    """
    Parse F32A block (or fallback to text) and return dict with:
    {'date_reference': iso-date or None, 'devise': 'USD'|'EUR'|..., 'montant': float or None}
    """
    blk = get_field_block(text, 'F32A') or text or ""
    blk_clean = re.sub(r'#.*?#', '', blk, flags=re.S)
    res = {'date_reference': None, 'devise': None, 'montant': None}

    # date: try explicit Date: 251222 or a 6-digit token
    m_date = re.search(r'(?i)\bDate[:\s]*([0-9]{6})\b', blk_clean)
    if m_date:
        res['date_reference'] = parse_date_YYMMDD(m_date.group(1))
    else:
        m_date2 = re.search(r'(\d{6})', blk_clean)
        if m_date2:
            res['date_reference'] = parse_date_YYMMDD(m_date2.group(1))

    # currency: try "Currency:" or "Devise:" or any 3-letter token contextually near amount
    m_cur = re.search(r'(?i)\b(?:Devise|Currency)[:\s\S]{0,40}?([A-Z]{3})\b', blk_clean)
    if m_cur:
        res['devise'] = m_cur.group(1).upper()
    else:
        m_cur2 = re.search(r'\b([A-Z]{3})\b', blk_clean)
        if m_cur2:
            res['devise'] = m_cur2.group(1).upper()

    # amount: prefer explicit "Montant|Amount" line
    candidate = None
    m_line = re.search(r'(?im)^\s*(?:Montant|Amount)\s*[:\-]\s*(.*)$', blk_clean, flags=re.M)
    if m_line:
        line = m_line.group(1).strip()
        nums = re.findall(r'([0-9]+(?:[.,\s][0-9]{1,3})*(?:[.,][0-9]{1,2})?)', line)
        if nums:
            def digits_len(s): return len(re.sub(r'[^0-9]', '', s))
            candidate = max(nums, key=digits_len)
    if not candidate:
        # fallback: pick the longest numeric-looking token in block
        nums_all = re.findall(r'([0-9]+(?:[.,\s][0-9]{1,3})*(?:[.,][0-9]{1,2})?)', blk_clean)
        if nums_all:
            def digits_len(s): return len(re.sub(r'[^0-9]', '', s))
            candidate = max(nums_all, key=digits_len)
    if candidate:
        res['montant'] = parse_amount(candidate)
    return res

def extract_receiver_bic(text: str) -> Optional[str]:
    """
    Try to extract the receiver BIC from header 'Receiver:' or anywhere in the text.
    Returns first matched BIC-like token or None.
    """
    if not text:
        return None
    # try 'Receiver:' block
    m = re.search(r'(?i)Receiver\s*[:\-]?\s*(.*?)(?=\n[A-Z][a-z]|$)', text, re.S)
    if m:
        part = m.group(1)
        m2 = BIC_RE.search(part)
        if m2:
            return m2.group(0)
    # fallback: search nearby 'RECEIVER' text region
    idx = text.upper().find('RECEIVER')
    if idx >= 0:
        tail = text[idx: idx + 400]
        m2 = BIC_RE.search(tail)
        if m2:
            return m2.group(0)
    # final fallback: any BIC-looking token in document
    m_any = BIC_RE.findall(text)
    return m_any[0] if m_any else None

# ---------- main extractor for text-block ----------
def extract_from_text(text: str, source: str = None) -> dict:
    row = {
        "type_MT": None,
        "code_banque": None,
        "sender_bic": None,
        "receiver_bic": None,
        "reference": None,
        "date_reference": None,
        "devise": None,
        "montant": None,
        "donneur_dordre": None,
        "beneficiaire": None,
        "pays_iso3": None,
        "source_pdf": source
    }

    # type_MT detection
    m = MT_RE.search(text)
    if m:
        row["type_MT"] = f"fin.{m.group(1)}".lower()

    # receiver BIC (prefer header)
    rb = extract_receiver_bic(text)
    row["code_banque"] = rb
    row["receiver_bic"] = rb

    # robust reference extraction : prefer F20 inside block4 if present
    block4 = get_field_block(text, 'Block 4') or get_field_block(text, 'Block4') or text
    # also try F20 block explicitly
    f20_block = get_field_block(text, 'F20') or get_field_block(text, ':20') or None
    # choose block4_text as f20_block if present else block4
    block4_text = f20_block or block4
    row["reference"] = extract_transaction_reference(text, block4_text)

    # parse amount/date/currency from F32A or text
    f32 = parse_f32a(text)
    row["date_reference"] = f32.get('date_reference')
    row["devise"] = f32.get('devise')
    row["montant"] = f32.get('montant')

    # F52A: use bic_utils.get_donneur_from_f52 to produce CODE/NAME or CODE
    f52_block = get_field_block(text, 'F52A') or get_field_block(text, 'F52A:')
    try:
        donneur = get_donneur_from_f52(f52_block or "", message_text=text)
    except Exception:
        donneur = None
    row["donneur_dordre"] = donneur

    # payer/beneficiary names: try F59/F58 blocks if present (simple best-effort)
    f59 = get_field_block(text, 'F59') or get_field_block(text, 'F58') or None
    if f59:
        # pick first non-empty line as beneficiary readable text
        lines = [ln.strip() for ln in f59.splitlines() if ln.strip()]
        if lines:
            row["beneficiaire"] = lines[0] if not row.get("beneficiaire") else row.get("beneficiaire")

    # country detection
    row["pays_iso3"] = detect_country_from_text(text)

    return row

def extract_block(block_text: str, source: str = None) -> dict:
    return extract_from_text(block_text, source=source)

def extract_for_mt202(pdf_path):
    txt = extract_text_from_pdf(pdf_path)
    return extract_from_text(txt, source=getattr(pdf_path, "name", str(pdf_path)))
=== ./dist/launcher/_internal/backend/app/extractors/mt910.py ===
# backend/app/extractors/mt910.py
"""
Extractor for SWIFT-like MT910 (confirmations/report outputs).
Rules (as requested):
 - For MT910, donor = sender, beneficiary = receiver.
 - Do NOT consult external BIC mapping for MT910.
 - Extract sender code (exactly 11 chars, A-Z0-9) from Sender Institution block.
 - Extract receiver code (exactly 11 chars) from Receiver Institution block.
 - Extract expansion name from "Expansion:" if present (same block) and produce "[CODE]/[NAME]".
 - If name missing, fall back to a readable line from the block.
 - Always set sender_bic and receiver_bic to the raw 11-char code (if found).
 - Ensure reference extraction still uses header "Transaction Reference" or block4 :20:.
"""

import re
from pathlib import Path
from typing import Optional
import logging

logger = logging.getLogger(__name__)

# reuse helpers from mt202 for consistent text extraction and small utilities
from backend.app.extractors.mt202 import (
    extract_text_from_pdf,
    parse_amount,
    parse_date_YYMMDD,
    detect_country_from_text,
    BIC_RE,
    get_field_block,
)

# ---------- helpers ----------

# strict sender/receiver code: uppercase letters/digits exactly 11 chars
CODE11_RE = re.compile(r'\b([A-Z0-9]{11})\b')

def _parse_block4(text: str) -> Optional[str]:
    m = re.search(r'(?si)Block\s*4(.*?)(?:Block\s*5|Message Text|End of report|End of Message|$)', text)
    return m.group(1).strip() if m else None

def _extract_tag_from_block4(block4: Optional[str], tag: str) -> Optional[str]:
    if not block4:
        return None
    pat = re.compile(r'(?m)^:' + re.escape(tag) + r':\s*(.*)$')
    m = pat.search(block4)
    if m:
        return m.group(1).strip()
    m2 = re.search(r':' + re.escape(tag) + r':\s*([^\r\n]+)', block4)
    return m2.group(1).strip() if m2 else None

def _extract_sender_receiver_header(text: str):
    """
    Extract the raw 'Sender Institution' and 'Receiver Institution' blocks.
    Returns (sender_block_text, receiver_block_text) or (None, None).
    """
    sender = None
    receiver = None
    m_sender = re.search(r'(?si)Sender Institution\s*[:\s]*([^\n].*?)(?=Receiver Institution\s*:|Message Text|Block 4|$)', text)
    if m_sender:
        sender = m_sender.group(1).strip()
    m_receiver = re.search(r'(?si)Receiver Institution\s*[:\s]*([^\n].*?)(?=Message Text|Block 4|$)', text)
    if m_receiver:
        receiver = m_receiver.group(1).strip()
    return sender, receiver

def _compact_whitespace(s: Optional[str]) -> Optional[str]:
    return re.sub(r'\s+', ' ', s).strip() if s else s

def _extract_expansion_name(block_text: str) -> Optional[str]:
    """
    Get the first 'Expansion:' value inside the header block (if present).
    Accepts 'Expansion: NAME' on same line.
    """
    if not block_text:
        return None
    m = re.search(r'(?i)Expansion\s*[:\s]\s*([^\r\n]+)', block_text)
    if m:
        return m.group(1).strip()
    # sometimes "Expansion: <name>" may be followed on next lines; try capture across small window
    m2 = re.search(r'(?i)Expansion\s*[:\s]\s*([^\n]{1,120})', block_text)
    if m2:
        return m2.group(1).strip()
    return None

def _find_code11_in_block(block_text: Optional[str]) -> Optional[str]:
    """
    Find the first token matching exactly 11 alnum (A-Z0-9) in the block.
    Uppercase result returned.
    """
    if not block_text:
        return None
    # search for 11-char token (prefer one containing letters)
    toks = re.findall(r'\b([A-Z0-9]{11})\b', block_text, flags=re.I)
    if not toks:
        return None
    # prefer token that contains a letter (likely BIC-type)
    for t in toks:
        if re.search(r'[A-Z]', t, flags=re.I):
            return t.upper()
    # else return first
    return toks[0].upper()

def _format_code_and_name(code: Optional[str], name: Optional[str]) -> Optional[str]:
    if not code and not name:
        return None
    if code:
        code_u = code.strip().upper()
        if name:
            return f"{code_u}/{_compact_whitespace(name)}"
        return code_u
    # no code but name present
    return _compact_whitespace(name)

# ---------- main extractor ----------

def _extract_from_text(text: str, source: str = None) -> dict:
    row = {
        "type_MT": "fin.910",
        "code_banque": None,
        "sender_bic": None,
        "receiver_bic": None,
        "reference": None,
        "date_reference": None,
        "devise": None,
        "montant": None,
        "donneur_dordre": None,      # legacy key expected by app
        "institution_name": None,    # alias for donneur
        "beneficiaire": None,
        "pays_iso3": None,
        "source_pdf": source,
        "related_reference": None,
        "sender_account": None
    }

    # 0) reference: prefer header "Transaction Reference" (works cross-block) then :20:
    m_tr = re.search(r'(?i)Transaction Reference\s*[:\s]*([A-Z0-9\-\_]+)', text)
    if m_tr:
        row["reference"] = m_tr.group(1).strip()

    # 1) header sender/receiver
    sender_blk, receiver_blk = _extract_sender_receiver_header(text)

    # sender
    if sender_blk:
        code = _find_code11_in_block(sender_blk)
        expansion = _extract_expansion_name(sender_blk)
        formatted = _format_code_and_name(code, expansion)
        if formatted:
            row["donneur_dordre"] = formatted
            row["institution_name"] = formatted
        else:
            # fallback: pick a readable line
            lines = [ln.strip() for ln in sender_blk.splitlines() if ln.strip()]
            if lines:
                # if first token looks like code, remove it from the name
                first = lines[0]
                # if first contains code at start, drop it for name
                mcode = CODE11_RE.search(first)
                if mcode:
                    candidate_name = " / ".join(lines[1:3]) if len(lines) > 1 else first
                else:
                    candidate_name = " / ".join(lines[:2])
                row["donneur_dordre"] = _compact_whitespace(candidate_name)
        # also set sender_bic as code if found
        if sender_blk:
            c11 = _find_code11_in_block(sender_blk)
            if c11:
                row["sender_bic"] = c11
                # set code_banque if not set
                if not row.get("code_banque"):
                    row["code_banque"] = c11

    # receiver
    if receiver_blk:
        code_r = _find_code11_in_block(receiver_blk)
        expansion_r = _extract_expansion_name(receiver_blk)
        formatted_r = _format_code_and_name(code_r, expansion_r)
        if formatted_r:
            row["beneficiaire"] = formatted_r
        else:
            lines = [ln.strip() for ln in receiver_blk.splitlines() if ln.strip()]
            if lines:
                # similar fallback
                first = lines[0]
                mcode = CODE11_RE.search(first)
                if mcode:
                    candidate_name = " / ".join(lines[1:3]) if len(lines) > 1 else first
                else:
                    candidate_name = " / ".join(lines[:2])
                row["beneficiaire"] = _compact_whitespace(candidate_name)
        if code_r:
            row["receiver_bic"] = code_r
            # if code_banque not set prefer receiver
            if not row.get("code_banque"):
                row["code_banque"] = code_r

    # 2) block4 tags: prefer :20: for reference, :32A: for date/currency/amount, :25P for account
    block4 = _parse_block4(text)
    if block4:
        tag20 = _extract_tag_from_block4(block4, '20')
        if tag20:
            row["reference"] = tag20.strip()
        tag21 = _extract_tag_from_block4(block4, '21')
        if tag21:
            row["related_reference"] = tag21.strip()
        tag25 = _extract_tag_from_block4(block4, '25P') or _extract_tag_from_block4(block4, '25')
        if tag25:
            row["sender_account"] = tag25.strip()
        tag32 = _extract_tag_from_block4(block4, '32A')
        if tag32:
            # reuse small parser from mt202: try simple inline parse
            m = re.match(r'^\s*(\d{6})\s*([A-Z]{3})\s*([0-9\.,]+)\s*$', tag32)
            if m:
                date_iso = parse_date_YYMMDD(m.group(1))
                cur = m.group(2).upper()
                try:
                    amt = float(m.group(3).replace('.', '').replace(',', '.'))
                except Exception:
                    amt = None
            else:
                # fallback: find currency token and number
                m2 = re.search(r'([A-Z]{3})\s*([0-9\.,]+)', tag32)
                if m2:
                    cur = m2.group(1).upper()
                    try:
                        amt = float(m2.group(2).replace('.', '').replace(',', '.'))
                    except Exception:
                        amt = None
                else:
                    date_iso = None
                    cur = None
                    amt = None
            if date_iso:
                row["date_reference"] = date_iso
            if cur:
                row["devise"] = cur
            if amt is not None:
                row["montant"] = amt

    # 3) fallback free text amount/date
    if row.get("montant") is None:
        m_amt = re.search(r'(?i)Amount[:\s]*([0-9\.,\s]+)\s*(?:Currency[:\s]*([A-Z]{3}))?', text)
        if m_amt:
            s = m_amt.group(1)
            cur = m_amt.group(2)
            try:
                val = float(s.replace('.', '').replace(',', '.'))
            except Exception:
                val = None
            row["montant"] = val
            if cur:
                row["devise"] = cur.upper()

    if not row.get("date_reference"):
        m_val = re.search(r'(?i)Value Date[:\s]*([0-3]?\d)[\/\-]([01]?\d)[\/\-]([0-9]{2,4})', text)
        if m_val:
            d, mth, y = m_val.group(1), m_val.group(2), m_val.group(3)
            if len(y) == 2:
                y = '20' + y
            try:
                row["date_reference"] = f"{int(y):04d}-{int(mth):02d}-{int(d):02d}"
            except Exception:
                pass

    # 4) country detection
    row["pays_iso3"] = detect_country_from_text(text)

    # normalization: uppercase currency
    if row.get("devise"):
        row["devise"] = row["devise"].upper()

    # cast montant to float if possible (already done)
    try:
        if row.get("montant") is not None:
            row["montant"] = float(row["montant"])
    except Exception:
        row["montant"] = None

    return row

# Public API
def extract_block(block_text: str, source: str = None) -> dict:
    return _extract_from_text(block_text, source=source)

def extract_for_mt910(pdf_path):
    p = Path(pdf_path)
    txt = extract_text_from_pdf(p)
    return _extract_from_text(txt, source=getattr(p, "name", str(p)))

if __name__ == "__main__":
    import sys
    from pprint import pprint
    if len(sys.argv) < 2:
        print("Usage: python3 mt910.py path/to/910.pdf")
        raise SystemExit(1)
    pprint(extract_for_mt910(sys.argv[1]))
=== ./dist/launcher/_internal/backend/app/extractors/mt_multi.py ===
"""
Dispatcher / découpeur de messages SWIFT.
- Lit un PDF (pdfplumber)
- Découpe en messages
- Détecte le type MT (202, 103, 910, 202.COV, ...)
- Appelle l'extracteur spécialisé (mt202, mt103, mt910)
- Pour 202.COV : utilise la même extraction que 202 pour tous les champs,
  mais met type_MT = "fin.202.COV" (d'après "Identifier: fin.202.COV" du header)
- Post-traitement: pour MT202/MT103 (et variantes), tente d'extraire le token strict
  depuis F52A et de formater "CODE/Bank Name" via bic_utils si disponible.
Returns list[dict] standardisés.
"""

from pathlib import Path
import re
from typing import List, Dict, Optional
import pdfplumber
import logging

logger = logging.getLogger(__name__)

# specialized extractors (block-level API: extract_block(block_text, source=...))
from backend.app.extractors import mt202, mt103, mt910

# optional bic mapping utilities (used only for 202/103 postprocessing)
try:
    from backend.app.extractors import bic_utils
    HAS_BIC_UTILS = True
except Exception:
    bic_utils = None
    HAS_BIC_UTILS = False

# ---------- patterns ----------
# try to capture "Identifier: fin.202.COV" (we will extract the tail e.g. "202" or "202.COV")
IDENTIFIER_FIN_FULL_RE = re.compile(r'(?i)Identifier\s*[:\s]*\s*fin\.(\d{3}(?:\.[A-Z0-9]+)?)')
# fallback simpler inline MT tokens
MT_INLINE_RE = re.compile(r'\b(?:FIN|MT)[\s\-\._:\/]*(\d{3})\b', re.I)

# small helper to get F52A from a block (try to reuse mt202 helper if present)
try:
    from backend.app.extractors.mt202 import get_field_block
except Exception:
    def get_field_block(text: str, field_label: str) -> Optional[str]:
        # crude fallback: find occurrences of the label and return following lines until next F.. or blank
        pat = re.compile(r'(?si)(' + re.escape(field_label) + r'[:\s]*)(.*?)(?=\nF\d{2}[A-Z]?:|\nF\d{2}\b|$)')
        m = pat.search(text)
        return m.group(2).strip() if m else None


def _safe_text_extract(pdf_path: Path) -> str:
    """
    Extract text reliably from pdf using pdfplumber and normalize newlines.
    Keep some whitespace structure (double newlines) but remove excessive blank runs.
    """
    text = ""
    with pdfplumber.open(str(pdf_path)) as pdf:
        for p in pdf.pages:
            text += "\n" + (p.extract_text() or "")
    # normalize
    text = text.replace('\r', '\n')
    # remove "page X of Y" lines often injected
    text = re.sub(r'(?mi)^\s*page\s+\d+\s*(?:of\s*\d+)?\s*$', '', text, flags=re.M)
    # collapse long empty runs to two newlines (keep paragraph separation)
    text = re.sub(r'\n{3,}', '\n\n', text)
    return text


def _split_messages(text: str) -> List[str]:
    """
    Robust splitting into messages. Try multiple heuristics because pdf text extraction
    can vary a lot between files.
    Returns list of message blocks (stripped).
    """
    if not text:
        return []

    txt = text.replace('\r', '\n')
    # keep original to use slices by index
    norm = txt

    # 1) 'Message N' headings (common in many dumps)
    msgs = list(re.finditer(r'(?m)^\s*Message\s+\d+\b', norm))
    if len(msgs) >= 2:
        positions = [m.start() for m in msgs] + [len(norm)]
        blocks = [norm[positions[i]:positions[i+1]].strip() for i in range(len(positions)-1)]
        # filter out empty
        return [b for b in blocks if b]

    # 2) 'Identifier: fin.XXX' header occurrences (covers fin.202.COV etc.)
    idents = list(re.finditer(r'(?mi)Identifier\s*[:\s]*fin\.\d{3}(?:\.[A-Z0-9]+)?', norm))
    if len(idents) >= 2:
        positions = [m.start() for m in idents] + [len(norm)]
        blocks = [norm[positions[i]:positions[i+1]].strip() for i in range(len(positions)-1)]
        return [b for b in blocks if b]

    # 3) 'Unique Message Identifier' / 'Message Identifier' headings
    umi = list(re.finditer(r'(?m)^(?:Unique Message Identifier|Message Identifier)\b', norm, flags=re.M))
    if len(umi) >= 2:
        positions = [m.start() for m in umi] + [len(norm)]
        blocks = [norm[positions[i]:positions[i+1]].strip() for i in range(len(positions)-1)]
        return [b for b in blocks if b]

    # 4) split by :20: / F20 tokens (more tolerant: any occurrence of :20: or F20: or F20)
    token_pat = re.compile(r'(?mi)(:20:|\bF20[:\s])')
    tokens = list(token_pat.finditer(norm))
    if tokens:
        positions = [m.start() for m in tokens]
        if positions and positions[0] != 0:
            positions = [0] + positions
        positions.append(len(norm))
        blocks = [norm[positions[i]:positions[i+1]].strip() for i in range(len(positions)-1)]
        # sometimes splitting on :20: yields an initial tiny prefix; drop very small blocks
        blocks = [b for b in blocks if len(b) > 10]
        if len(blocks) >= 2:
            return blocks

    # 5) visual separators like lines with '***' or '---'
    sep_matches = list(re.finditer(r'(?m)^\s*(\*{3,}|-{3,})\s*$', norm))
    if sep_matches:
        positions = []
        # collect segment between separators
        prev = 0
        blocks = []
        for m in sep_matches:
            s = norm[prev:m.start()].strip()
            if s:
                blocks.append(s)
            prev = m.end()
        tail = norm[prev:].strip()
        if tail:
            blocks.append(tail)
        if len(blocks) >= 2:
            return blocks

    # 6) fallback: try splitting by large page-like separators (multiple underscores)
    page_like = re.split(r'(?m)^\s*_{5,}\s*$', norm)
    if len(page_like) >= 2:
        blocks = [p.strip() for p in page_like if p.strip()]
        if len(blocks) >= 2:
            return blocks

    # final fallback: whole text as single block
    return [norm.strip()]


def _detect_mt_type(block_text: str) -> Optional[str]:
    """
    Detect specific MT type string:
      - prefer Identifier header form -> returns e.g. '202', '202.COV', '910'
      - else fallback to inline MT/FIN token -> returns digits like '202'
    """
    if not block_text:
        return None
    m = IDENTIFIER_FIN_FULL_RE.search(block_text)
    if m:
        return m.group(1)  # e.g. "202" or "202.COV"
    m2 = MT_INLINE_RE.search(block_text)
    if m2:
        return m2.group(1)
    return None


# ---------- postprocessing for 202/103: F52A -> CODE/Name ----------
def _postprocess_row_for_202_103(row: Dict, block_text: str, xlsx_path: Optional[str] = None) -> Dict:
    """
    For MT202 / MT103 and variants (like 202.COV) : attempt to extract a strict Identifier
    token from F52A (or message text) using bic_utils.get_donneur_from_f52 (if available).
    If a CODE or CODE/Name is found, fill row['donneur_dordre'] and row['institution_name'] and
    set row['code_banque'] if missing.
    """
    try:
        f52_block = get_field_block(block_text, 'F52A')
    except Exception:
        f52_block = None

    code_name = None
    code_only = None

    if HAS_BIC_UTILS:
        try:
            # bic_utils.get_donneur_from_f52 returns "CODE/Name" or CODE or None
            code_name = bic_utils.get_donneur_from_f52(f52_block, message_text=block_text, xlsx_path=xlsx_path)
        except Exception as e:
            logger.debug("mt_multi: bic_utils.get_donneur_from_f52 error: %s", e)
            code_name = None

    if not code_name:
        # fallback naive search near label if bic_utils absent or returned None
        m_label = re.search(r'(?i)(?:IdentifierCode|Identifier Code|Code d\'identifiant|Code d identifiant|Identifier code)\s*[:\-\s]*', block_text)
        if m_label:
            tail = block_text[m_label.end(): m_label.end() + 800]
            m_tok = re.search(r'\b([A-Z0-9]{8,11})\b', tail, flags=re.I)
            if m_tok:
                code_only = m_tok.group(1).upper()
                if HAS_BIC_UTILS:
                    try:
                        name = bic_utils.map_code_to_name(code_only, xlsx_path=xlsx_path)
                    except Exception:
                        name = None
                    code_name = f"{code_only}/{name}" if name else code_only
                else:
                    code_name = code_only

    if code_name:
        # store
        if '/' in code_name:
            code_only = code_name.split('/', 1)[0]
        else:
            code_only = code_name
        row["donneur_dordre"] = code_name
        row["institution_name"] = code_name
        if not row.get("code_banque"):
            row["code_banque"] = code_only
    return row


def extract_messages_from_pdf(pdf_path: Path, bic_xlsx: Optional[str] = None) -> List[Dict]:
    """
    Main entrypoint: read pdf_path, split into messages, dispatch to extractors.
    bic_xlsx: optional path forwarded to bic_utils when used in postprocessing.
    """
    pdf_path = Path(pdf_path)
    if not pdf_path.exists():
        raise FileNotFoundError(pdf_path)

    # if bic_utils available, try to preload mapping (best-effort)
    if HAS_BIC_UTILS:
        try:
            bic_utils.load_bic_mapping(bic_xlsx)
        except Exception as e:
            logger.debug("mt_multi: bic mapping preload failed: %s", e)

    text = _safe_text_extract(pdf_path)
    blocks = _split_messages(text)
    multi = len(blocks) > 1
    rows: List[Dict] = []

    for i, blk in enumerate(blocks, start=1):
        source_label = f"{pdf_path.name}#{i}" if multi else pdf_path.name
        mt_type_token = _detect_mt_type(blk)  # e.g. '202', '202.COV', '910'
        row: Optional[Dict] = None

        try:
            if mt_type_token and mt_type_token.startswith('202'):
                # includes '202' and variants like '202.COV'
                row = mt202.extract_block(blk, source=source_label)
                # postprocess like other 202/103
                row = _postprocess_row_for_202_103(row, blk, xlsx_path=bic_xlsx)

                # FORCE beneficiary empty for 202 variants (requirement)
                try:
                    row["beneficiaire"] = None
                except Exception:
                    row.update({"beneficiaire": None})

                # if variant .COV present, force type_MT accordingly
                if '.' in mt_type_token:
                    # example: mt_type_token == '202.COV' -> type_MT 'fin.202.COV'
                    row['type_MT'] = f"fin.{mt_type_token}"
                else:
                    row.setdefault('type_MT', 'fin.202')

            elif mt_type_token == '103':
                row = mt103.extract_block(blk, source=source_label)
                row = _postprocess_row_for_202_103(row, blk, xlsx_path=bic_xlsx)
            elif mt_type_token == '910':
                # For 910 we do NOT use bic mapping in dispatcher; mt910 is responsible
                row = mt910.extract_block(blk, source=source_label)
            else:
                # unknown: try mt202 then mt103 then mt910 as fallbacks (keeps existing behavior)
                try:
                    row = mt202.extract_block(blk, source=source_label)
                    row = _postprocess_row_for_202_103(row, blk, xlsx_path=bic_xlsx)
                except Exception:
                    try:
                        row = mt103.extract_block(blk, source=source_label)
                        row = _postprocess_row_for_202_103(row, blk, xlsx_path=bic_xlsx)
                    except Exception:
                        try:
                            row = mt910.extract_block(blk, source=source_label)
                        except Exception:
                            row = {
                                "type_MT": None,
                                "code_banque": None,
                                "sender_bic": None,
                                "receiver_bic": None,
                                "reference": None,
                                "date_reference": None,
                                "devise": None,
                                "montant": None,
                                "donneur_dordre": None,
                                "beneficiaire": None,
                                "pays_iso3": None,
                                "source_pdf": source_label
                            }
        except Exception as e:
            logger.exception("mt_multi: extractor failed for message %s (detected=%s): %s", source_label, mt_type_token, e)
            row = {
                "type_MT": f"fin.{mt_type_token}" if mt_type_token else None,
                "code_banque": None,
                "reference": None,
                "date_reference": None,
                "devise": None,
                "montant": None,
                "donneur_dordre": None,
                "beneficiaire": None,
                "pays_iso3": None,
                "source_pdf": source_label,
                "error": str(e)
            }

        # ensure expected keys present
        expected = ["type_MT","code_banque","sender_bic","receiver_bic","reference","date_reference",
                    "devise","montant","donneur_dordre","beneficiaire","pays_iso3","source_pdf"]
        for k in expected:
            if k not in row:
                row[k] = None
        if not row.get("source_pdf"):
            row["source_pdf"] = source_label

        rows.append(row)

    return rows


# quick CLI for manual test
if __name__ == "__main__":
    import sys
    from pprint import pprint
    if len(sys.argv) < 2:
        print("Usage: python mt_multi.py path/to/all.pdf")
        raise SystemExit(1)
    path = Path(sys.argv[1])
    pprint(extract_messages_from_pdf(path))
=== ./streamlit_app/app.py ===
# streamlit_app/app.py
# Interface Streamlit pour l'extracteur PDF SWIFT
import sys
from pathlib import Path
import tempfile
import shutil
import io
import traceback

import streamlit as st
import pandas as pd

# --- make project root importable and prefer 'backend' package ---
ROOT = Path(__file__).resolve().parents[1]   # project root: pdf-extractor
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

# import backend functions (extractor manager)
try:
    # import extract_single, create_workbook and extract_dispatch in one place
    from backend.app.extractor_manager import extract_single, create_workbook, extract_dispatch
except Exception as e:
    st.error(f"Impossible d'importer l'extracteur backend: {e}")
    st.stop()

from backend.app.extractors import bic_utils

try:
    m = bic_utils.load_bic_mapping()    # retourne dict
    st.write("BIC map size:", len(m))
except Exception as e:
    st.write("BIC load failed:", e)

# UI configuration
st.set_page_config(page_title="PDF SWIFT Extractor (GUI)", layout="wide")
st.title("PDF SWIFT Extractor — Interface clic-clic")

st.markdown(
    """
    **Mode d'emploi rapide**
    - Glisser-déposer un ou plusieurs fichiers PDF ci-dessous.
    - Cliquez sur **Extraire**. Les fichiers sont analysés localement.
    - Téléchargez le workbook Excel ou enregistrez-le sur le serveur.
    """
)

# File uploader (multiple)
uploaded_files = st.file_uploader("Choisir des fichiers PDF", type="pdf", accept_multiple_files=True)

col1, col2 = st.columns([1, 1])
with col1:
    save_mode = st.radio("Mode de sortie", ("Télécharger le workbook", "Enregistrer sur le serveur (output/tables)"))

with col2:
    custom_out = st.text_input("Chemin de sortie (optionnel pour enregistrement serveur)", value=str(ROOT / "output" / "tables"))

run_button = st.button("Extraire")

# Logs viewer
with st.expander("Afficher les derniers logs"):
    log_file = ROOT / "logs" / "app.log"
    if log_file.exists():
        try:
            txt = log_file.read_text(encoding="utf-8")
            lines = txt.strip().splitlines()[-400:]
            st.text_area("logs/app.log (tail)", value="\n".join(lines), height=300)
        except Exception as e:
            st.write("Impossible de lire le fichier de logs:", e)
    else:
        st.write("Aucun fichier de logs trouvé (logs/app.log).")

# helper: save uploaded file to temp path and return Path
def save_uploaded_to_temp(uploaded) -> Path:
    tmpdir = Path(tempfile.mkdtemp(prefix="pdf_extr_"))
    dest = tmpdir / uploaded.name
    with open(dest, "wb") as f:
        f.write(uploaded.getbuffer())
    return dest

if run_button:
    if not uploaded_files:
        st.warning("Aucun fichier sélectionné.")
    else:
        rows = []
        progress = st.progress(0)
        total = len(uploaded_files)
        idx = 0
        errors = []
        tmp_dirs = []  # to cleanup later
        st.info(f"Lancement de l'extraction pour {total} fichier(s)...")

        for uf in uploaded_files:
            idx += 1
            st.write(f"Traitement : **{uf.name}** ({idx}/{total})")
            try:
                tmp_path = save_uploaded_to_temp(uf)
                tmp_dirs.append(tmp_path.parent)
            except Exception as e:
                errors.append((uf.name, f"Impossible d'enregistrer temporairement: {e}"))
                st.error(f"Impossible d'enregistrer temporairement {uf.name}: {e}")
                progress.progress(int(idx / total * 100))
                continue

            try:
                # extract_dispatch retourne toujours une LISTE de rows (1 ou plusieurs)
                new_rows = extract_dispatch(tmp_path)

                # Normalisations/garanties : mêmes clés pour chaque row, et source_pdf bien renseigné
                for r in new_rows:
                    # garantir la clé 'beneficiaire'
                    if "beneficiaire" not in r:
                        r["beneficiaire"] = None

                    # mapping backward-compatible pour 'donneur_dordre' si l'extracteur a renvoyé 'institution_name'
                    if "donneur_dordre" not in r:
                        if "institution_name" in r and r["institution_name"]:
                            r["donneur_dordre"] = r.get("institution_name")
                        else:
                            r["donneur_dordre"] = None

                    # s'assurer d'un source_pdf correct :
                    # - si l'extracteur n'a pas rempli source_pdf (rare), utiliser le nom du fichier uploadé
                    if not r.get("source_pdf"):
                        r["source_pdf"] = uf.name

                    # pour sécurité, si type_MT est None, on met un placeholder
                    if not r.get("type_MT"):
                        r["type_MT"] = None

                    rows.append(r)

                # message utilisateur synthétique
                types = sorted({rr.get("type_MT") or "type inconnu" for rr in new_rows})
                st.success(f"OK : {len(new_rows)} message(s) traité(s) — types : {', '.join(types)}")

            except Exception as e:
                tb = traceback.format_exc()
                errors.append((uf.name, str(e)))
                st.error(f"Erreur pendant l'extraction de {uf.name} : {e}")
                # affichage du traceback pour debug (expandable)
                with st.expander(f"Détails erreur pour {uf.name}"):
                    st.text(tb)
            progress.progress(int(idx / total * 100))

        # cleanup temp dirs
        for d in tmp_dirs:
            try:
                shutil.rmtree(d, ignore_errors=True)
            except Exception:
                pass

        progress.empty()

        # assemble display DataFrame (map internal keys -> user-facing labels)
        if rows:
            display_rows = []
            for r in rows:
                display_rows.append({
                    "code_banque": r.get("code_banque"),
                    "date_reference": r.get("date_reference"),
                    "reference": r.get("reference"),
                    "type_MT": r.get("type_MT"),
                    "pays_iso3": r.get("pays_iso3"),
                    # use the new key extracted by mt_multi
                    "donneur d'ordre": r.get("donneur_dordre"),
                    "Bénéficiaire": r.get("beneficiaire"),
                    "montant": r.get("montant"),
                    "devise": r.get("devise"),
                    "source_pdf": r.get("source_pdf")
                })

            df = pd.DataFrame(display_rows)

            # format montant column for display (no permanent change to rows)
            if "montant" in df.columns:
                try:
                    df["montant"] = df["montant"].apply(lambda x: ("{:,}".format(x)).replace(",", " ") if pd.notnull(x) else x)
                except Exception:
                    pass

            st.success("Extraction terminée — aperçu ci-dessous")
            st.dataframe(df, use_container_width=True)

            # Ensure backward-compatibility: create_workbook expects 'institution_name'
            for r in rows:
                if not r.get("institution_name"):
                    r["institution_name"] = r.get("donneur_dordre")

            # create workbook and either offer download or save on server
            if save_mode == "Télécharger le workbook":
                # create workbook in a temp directory and provide download
                temp_outdir = Path(tempfile.mkdtemp(prefix="swift_out_"))
                try:
                    out_path = create_workbook(rows, temp_outdir)  # returns Path to created workbook
                    with open(out_path, "rb") as f:
                        data = f.read()
                    st.download_button(
                        label="Télécharger le workbook Excel",
                        data=data,
                        file_name=out_path.name,
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                    )
                    st.info(f"Workbook généré: {out_path.name} (temp)")
                except Exception as e:
                    st.error(f"Impossible de créer le workbook: {e}")
                finally:
                    # optional: remove temp_outdir after offering download (download keeps data in browser)
                    try:
                        shutil.rmtree(temp_outdir, ignore_errors=True)
                    except Exception:
                        pass
            else:
                # save on server (custom_out or default)
                outdir = Path(custom_out) if custom_out else (ROOT / "output" / "tables")
                outdir.mkdir(parents=True, exist_ok=True)
                try:
                    outpath = create_workbook(rows, outdir)
                    st.success(f"Workbook enregistré : {outpath}")
                    st.write("Fichiers présents dans", outdir)
                    st.write(sorted([p.name for p in outdir.glob("*.xlsx")], reverse=True))
                except Exception as e:
                    st.error(f"Impossible d'enregistrer le workbook sur le serveur: {e}")

        else:
            st.warning("Aucun résultat extrait. Vérifiez le format des PDFs ou les logs.")

        # show errors list if any
        if errors:
            st.markdown("### Erreurs rencontrées")
            for name, msg in errors:
                st.write(f"- **{name}** : {msg}")

        st.info("Opération terminée.")
=== ./backend/app/main.py ===
# backend/app/main.py
from fastapi import FastAPI
from .api import router as api_router

app = FastAPI(title="PDF Swift Extractor API", version="0.1")
app.include_router(api_router, prefix="")
=== ./backend/app/db.py ===
# backend/app/db.py  (pbkdf2_sha256 backend to avoid bcrypt binary issues)
import os
from sqlalchemy import create_engine, Column, Integer, String
from sqlalchemy.orm import sessionmaker, declarative_base
from passlib.context import CryptContext

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATABASE_URL = os.environ.get("DATABASE_URL", f"sqlite:///{os.path.join(BASE_DIR, 'app.db')}")

engine = create_engine(DATABASE_URL, connect_args={"check_same_thread": False} if DATABASE_URL.startswith("sqlite") else {})
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()

# Use pbkdf2_sha256 (pure-python, no bcrypt C extension needed)
pwd_context = CryptContext(schemes=["pbkdf2_sha256"], deprecated="auto")

class User(Base):
    __tablename__ = "users"
    id = Column(Integer, primary_key=True, index=True)
    username = Column(String, unique=True, index=True, nullable=False)
    hashed_password = Column(String, nullable=False)
    role = Column(String, default="user")

def init_db():
    Base.metadata.create_all(bind=engine)

def get_user(db, username: str):
    return db.query(User).filter(User.username == username).first()

def create_user(db, username: str, password: str, role: str = "user"):
    # truncate very long passwords defensively
    if password is None:
        password = ""
    if isinstance(password, str):
        pw = password
    else:
        pw = str(password)
    # pbkdf2 supports long passwords; but keep reasonable length
    pw = pw[:1024]
    hashed = pwd_context.hash(pw)
    user = User(username=username, hashed_password=hashed, role=role)
    db.add(user)
    db.commit()
    db.refresh(user)
    return user

def verify_password(plain, hashed):
    return pwd_context.verify(plain, hashed)
=== ./backend/app/extractor_manager.py ===
# backend/app/extractor_manager.py
import os
import sys
from pathlib import Path
import re
import logging
from datetime import datetime
from typing import List, Dict, Optional

from openpyxl import Workbook
from openpyxl.utils import get_column_letter

from .utils import logger

# import extractors (primary helpers)
from .extractors.mt202 import extract_for_mt202, extract_text_from_pdf as extract_text_mt202
try:
    from .extractors.mt103 import extract_for_mt103
    HAS_MT103 = True
except Exception:
    HAS_MT103 = False
    logger.info("mt103 extractor not available at import time; you can add it to EXTRACTOR_MAP later.")

try:
    from .extractors.mt910 import extract_for_mt910
    HAS_MT910 = True
except Exception:
    HAS_MT910 = False
    logger.info("mt910 extractor not available at import time.")

# try to import the multi-message extractor (optional)
try:
    from .extractors import mt_multi as mt_multi_module
    HAS_MT_MULTI = True
except Exception:
    mt_multi_module = None
    HAS_MT_MULTI = False
    logger.info("mt_multi extractor not available at import time; multi-file detection will fall back to single extractors.")

# regex to detect MT/FIN code
MT_DETECT = re.compile(r'\b(?:MT|FIN)[\s\-\_\.:\/]*(\d{3})\b', re.I)
IDENTIFIER_FIN_RE = re.compile(r'Identifier[:\s]*fin[\.:\s\-\/]*(\d{3})', re.I)

# map MT number -> extractor callable
EXTRACTOR_MAP = {
    "202": extract_for_mt202,
}
if HAS_MT103:
    EXTRACTOR_MAP["103"] = extract_for_mt103
if HAS_MT910:
    EXTRACTOR_MAP["910"] = extract_for_mt910

# -----------------------------
# BIC mapping / donor logic
# -----------------------------
# caching globals
_cached_mapping: Optional[Dict[str, str]] = None
_cached_mapping_path: Optional[str] = None

# heuristics: possible default file locations
_DEFAULT_XLS_PATHS = [
    "data/bic_codes.xlsx",
    "data/bic.xlsx",
    "bic_codes.xlsx",
    "bic.xlsx",
    "data/bfde98b8-0a94-4ba1-ab8a-eae27357cc7e.xlsx"  # the uploaded name you used earlier (kept as candidate)
]

def _find_columns(df):
    """
    Find likely code_col and name_col heuristically from DataFrame columns.
    """
    cols = list(df.columns)
    code_col = None
    name_col = None
    for c in cols:
        cu = c.upper()
        if ('BIC' in cu) or (('CODE' in cu) and ('BIC' in cu or 'SWIFT' in cu)):
            code_col = c
            break
    if not code_col:
        for c in cols:
            cu = c.upper()
            if 'CODE' in cu:
                code_col = c
                break

    for c in cols:
        cu = c.upper()
        if 'NOM' in cu or 'NAME' in cu or 'NOMS' in cu:
            name_col = c
            break
    if not name_col:
        candidate = None
        best_alpha = 0.0
        for c in cols:
            sample = ' '.join([str(x) for x in df[c].dropna().astype(str).head(20).tolist()])
            if not sample:
                continue
            alpha_frac = sum(ch.isalpha() for ch in sample) / max(1, len(sample))
            if alpha_frac > best_alpha:
                best_alpha = alpha_frac
                candidate = c
        name_col = candidate
    return code_col, name_col


def bundled_base_path() -> Path:
    """
    Retourne le chemin racine d'où lire les fichiers "embarqués".
    - Si l'app est packagée par PyInstaller (--onefile), les resources sont extraites
      temporairement dans sys._MEIPASS.
    - Sinon, retourne la racine du projet (deux niveaux au-dessus de ce fichier).
    """
    if getattr(sys, "frozen", False) and hasattr(sys, "_MEIPASS"):
        return Path(sys._MEIPASS)
    # adjust parents count depending on file location; this file is backend/app/extractor_manager.py
    # parents[2] points to repo root (pdf-extractor/)
    return Path(__file__).resolve().parents[2]

def _user_override_bic_paths() -> List[Path]:
    """
    Emplacements où un admin/utilisateur peut déposer un bic_codes.xlsx modifiable.
    Ordre de priorité (testé dans load_bic_mapping) :
      1) variable d'environnement PDF_SWIFT_DATA_DIR si définie
      2) dossier commun ProgramData (Windows) -> {PROGRAMDATA}/PDF_Swift_Extractor/data
      3) dossier local de l'utilisateur -> %LOCALAPPDATA%/PDF_Swift_Extractor/data ou ~/ .pdf_swift_extractor/data
    """
    paths = []
    env = os.getenv("PDF_SWIFT_DATA_DIR")
    if env:
        paths.append(Path(env))

    # Windows common appdata (ProgramData)
    programdata = os.getenv("PROGRAMDATA")
    if programdata:
        paths.append(Path(programdata) / "PDF_Swift_Extractor" / "data")

    # Windows local appdata or cross-platform user dir
    localappdata = os.getenv("LOCALAPPDATA") or os.getenv("XDG_DATA_HOME")
    if localappdata:
        paths.append(Path(localappdata) / "PDF_Swift_Extractor" / "data")

    # fallback to user home hidden dir
    paths.append(Path.home() / ".pdf_swift_extractor" / "data")

    return paths

def load_bic_mapping(xlsx_path: Optional[str] = None, sheet_name: Optional[str] = 0) -> Dict[str, str]:
    """
    Charge (et met en cache) la table BIC -> nom de banque depuis un fichier Excel.
    Logique améliorée pour permettre une mise à jour manuelle après installation :
      - si xlsx_path explicit fourni -> utilisé (existing behaviour)
      - sinon : on cherche d'abord dans des emplacements externes éditables (ProgramData, user dir,
        variable d'env PDF_SWIFT_DATA_DIR)
      - sinon : on cherche dans le bundle embarqué (bundled_base_path()/data)
      - sinon : on retombe sur les chemins _DEFAULT_XLS_PATHS comme avant
    """
    global _cached_mapping, _cached_mapping_path
    import pandas as pd  # lazy import

    # 1) if explicit path provided, prefer it
    if xlsx_path:
        p = Path(xlsx_path)
        if not p.exists():
            raise FileNotFoundError(f"Provided xlsx_path not found: {xlsx_path}")
    else:
        # 2) check user-writable override locations (ProgramData, LOCALAPPDATA, env var)
        p = None
        for base in _user_override_bic_paths():
            candidate = base / "bic_codes.xlsx"
            if candidate.exists():
                p = candidate
                break
            # also accept alternative names
            alt = base / "bic.xlsx"
            if alt.exists():
                p = alt
                break

        # 3) check bundled data dir (this will work for --onedir and for files added with --add-data)
        if p is None:
            bundled = bundled_base_path() / "data"
            for name in ("bic_codes.xlsx", "bic.xlsx"):
                cand = bundled / name
                if cand.exists():
                    p = cand
                    break

        # 4) fallback to original candidate list (relative to current working dir)
        if p is None:
            for cand in _DEFAULT_XLS_PATHS:
                if Path(cand).exists():
                    p = Path(cand)
                    break

        if p is None:
            raise FileNotFoundError(
                "Aucun fichier Excel trouvé. Place your Excel mapping in one of: "
                + ", ".join(_DEFAULT_XLS_PATHS)
                + " or a writable location like %PROGRAMDATA%\\PDF_Swift_Extractor\\data\\bic_codes.xlsx "
                + "or set environment variable PDF_SWIFT_DATA_DIR to a folder containing bic_codes.xlsx"
            )

    # cache check
    pstr = str(Path(p).resolve())
    if _cached_mapping is not None and _cached_mapping_path == pstr:
        return _cached_mapping

    df = pd.read_excel(pstr, sheet_name=sheet_name, dtype=str)
    df = df.dropna(axis=1, how='all')

    code_col, name_col = _find_columns(df)
    if not code_col:
        raise ValueError(f"Impossible de détecter la colonne code BIC dans {pstr}. Colonnes: {list(df.columns)}")
    if not name_col:
        logger.warning("load_bic_mapping: impossible de détecter colonne 'nom' ; les valeurs de nom seront vides.")

    mapping: Dict[str, str] = {}
    for _, row in df.iterrows():
        code_val = (str(row.get(code_col) or "")).strip()
        name_val = (str(row.get(name_col) or "")).strip() if name_col else ""
        if not code_val or code_val.lower() in ("nan", "none"):
            continue
        code_clean = re.sub(r'\s+', '', code_val).upper()
        key = code_clean[:8]
        if not key:
            continue
        mapping[key] = name_val

    _cached_mapping = mapping
    _cached_mapping_path = pstr
    logger.info("load_bic_mapping: loaded %d entries from %s", len(mapping), pstr)
    return mapping


FALLBACK_11_RE = re.compile(r'\b([A-Z0-9]{11})\b')

# remplace la fonction get_donneur_from_f52 existante par ceci
IDENTIFIER_RE_AFTER_LABEL = re.compile(
    r"(?i)(?:IdentifierCode|Identifier Code|Identifiercode|Code d'identifiant|Code d identifiant|IDENTIFIERCODE)\s*[:\-\s]*\n?\s*([A-Z0-9]{11})"
)

# words that look like labels and should NOT be accepted as code
_BAD_LABEL_TOKENS = {
    "IDENTIFIER", "IDENTIFIERC", "PARTYIDENTI", "PARTYIDENT", "IDENTIFIANT", "IDENTIFIERCODE",
    "PARTY", "PARTYIDENTIFIER"
}


# label regex (variantes FR/EN)
_LABEL_RE = re.compile(
    r"(?i)(?:IdentifierCode|Identifier Code|Identifiercode|Code d'identifiant|Code d identifiant|identifiant de partie|IDENTIFIERCODE)\s*[:\-\s]*",
    re.M
)

_BAD_LABEL_PREFIXES = ("IDENTIF", "PARTYIDENT", "PARTY", "IDENTIFIANT")

def _find_identifier_after_label(text: str, lookahead_chars: int = 600) -> Optional[str]:
    """
    Cherche après un label 'IdentifierCode' un token alphanumérique 6..11 caractères,
    autorise lignes vides entre label et token, privilégie tokens contenant des lettres.
    """
    if not text:
        return None
    txt = text.replace('\r', '\n')
    m = _LABEL_RE.search(txt)
    if not m:
        return None
    start = m.end()
    tail = txt[start: start + lookahead_chars]
    # find candidates 6..11 chars
    toks = re.findall(r'\b([A-Z0-9]{6,11})\b', tail, flags=re.I)
    toks = [t.upper() for t in toks]
    # filter label-like tokens
    toks = [t for t in toks if not any(t.startswith(pref) for pref in _BAD_LABEL_PREFIXES)]
    if not toks:
        return None
    # prefer token with a letter (likely BIC), otherwise return first
    for t in toks:
        if re.search(r'[A-Z]', t):
            return t
    return toks[0]

def get_donneur_from_f52(f52_text: Optional[str], message_text: Optional[str] = None, xlsx_path: Optional[str] = None) -> Optional[str]:
    """
    Robust extract:
     - try inside F52A block: find label then next token (6..11 chars), preferring alpha tokens
     - if not found, try search on the whole message (cross-page)
     - if still not found, attempt a last-resort token in F52A that contains letters
     - then map first 8 chars to bank name using load_bic_mapping (if available)
    Returns: "CODE11/BANK NAME" if mapping found, else CODE (or None).
    """
    # normalize and remove xml-like tags
    def _norm(s):
        return re.sub(r'<[^>]+>', ' ', (s or "")).replace('\r', '\n')

    f52 = _norm(f52_text)
    full = _norm(message_text) if message_text else None

    # 1) try strictly in F52A block
    code = _find_identifier_after_label(f52, lookahead_chars=800)
    # 2) if not found, try whole message (cross-page)
    if not code and full:
        code = _find_identifier_after_label(full, lookahead_chars=1200)
    # 3) last-resort: try to find first alnum token with letters in F52A
    if not code and f52:
        m = re.search(r'\b([A-Z][A-Z0-9]{5,10})\b', f52, flags=re.I)
        if m:
            tok = m.group(1).upper()
            if not any(tok.startswith(pref) for pref in _BAD_LABEL_PREFIXES):
                code = tok

    if not code:
        return None

    # attempt mapping to bank name (uses cached loader)
    try:
        mapping = load_bic_mapping(xlsx_path=xlsx_path)
    except Exception:
        mapping = {}

    key8 = code[:8].upper()
    bank = mapping.get(key8)
    if bank:
        bank_clean = re.sub(r'\s{2,}', ' ', bank).strip()
        return f"{code}/{bank_clean}"
    return code


# -----------------------------
# Dispatcher / workbook logic (existing)
# -----------------------------
def detect_message_type(text: str) -> Optional[str]:
    """
    Detect the MT type (e.g. "202", "103", "910") from extracted text.
    Returns the numeric string (e.g. "202") or None.
    """
    if not text:
        return None

    m = MT_DETECT.search(text)
    if m:
        mt = m.group(1)
        logger.debug("detect_message_type: primary MT_DETECT matched -> %s", mt)
        return mt

    m2 = IDENTIFIER_FIN_RE.search(text)
    if m2:
        mt = m2.group(1)
        logger.debug("detect_message_type: IDENTIFIER_FIN_RE matched -> %s", mt)
        return mt

    logger.debug("detect_message_type: no MT type matched")
    return None


def extract_dispatch(pdf_path: Path) -> List[Dict]:
    """
    Dispatcher intelligent :
      - si le PDF contient plusieurs messages -> utilise mt_multi.extract_messages_from_pdf
      - sinon -> utilise extract_single (retourne [row])
    Retourne toujours une LISTE de rows.
    """
    p = Path(pdf_path)
    # quick text extraction using existing helper
    text = ""
    try:
        text = extract_text_mt202(p)
    except Exception as e:
        logger.debug("extract_dispatch: extract_text_mt202 failed (%s), falling back to pdfplumber", e)
        try:
            import pdfplumber
            s = ""
            with pdfplumber.open(str(p)) as pdf:
                for page in pdf.pages[:2]:
                    s += "\n" + (page.extract_text() or "")
            text = s
        except Exception as e2:
            logger.warning("extract_dispatch: quick pdfplumber fallback failed for %s: %s", p.name, e2)
            text = ""

    # If multi-message extractor available, use its split logic to decide
    if HAS_MT_MULTI and mt_multi_module:
        try:
            blocks = mt_multi_module._split_messages(text)
            if blocks and len(blocks) > 1:
                logger.info("%s: detected %d messages (using mt_multi).", p.name, len(blocks))
                rows = mt_multi_module.extract_messages_from_pdf(p)
                # ensure backward compatibility: set institution_name from donneur_dordre if missing
                for r in rows:
                    if "institution_name" not in r or not r.get("institution_name"):
                        r["institution_name"] = r.get("donneur_dordre") or r.get("donneur d'ordre") or None
                    for k in ["code_banque", "date_reference", "reference", "type_MT", "pays_iso3", "beneficiaire", "montant", "devise", "source_pdf"]:
                        if k not in r:
                            r[k] = None
                return rows
        except Exception as e:
            logger.exception("extract_dispatch: mt_multi detection/extraction failed for %s: %s", p.name, e)
            # fall through to single extractor

    # fallback: treat as single message
    single_row = extract_single(p)
    return [single_row]


def _ensure_minimal_row(p: Path, mt_type: Optional[str] = None) -> Dict:
    """Return a minimal row template used when extraction not performed or failed."""
    return {
        "code_banque": None,
        "date_reference": None,
        "reference": None,
        "type_MT": f"fin.{mt_type}" if mt_type else None,
        "pays_iso3": None,
        "institution_name": None,
        "beneficiaire": None,
        "montant": None,
        "devise": None,
        "source_pdf": p.name
    }


def extract_single(pdf_path: Path) -> Dict:
    """
    Dispatch extraction for a single pdf_path (Path or str).
    Returns a dict with fields (internal keys). The create_workbook function maps
    'institution_name' -> "donneur d'ordre" when writing the summary sheet.
    """
    p = Path(pdf_path)
    if not p.exists():
        logger.error("extract_single: file not found: %s", p)
        return _ensure_minimal_row(p)

    # read text (use helper from mt202 for consistent behavior)
    try:
        text = extract_text_mt202(p)
    except Exception as e:
        logger.exception("extract_single: extract_text_mt202 failed for %s: %s", p.name, e)
        # fallback quick text extraction
        try:
            import pdfplumber
            s = ""
            with pdfplumber.open(str(p)) as pdf:
                for page in pdf.pages[:2]:
                    s += "\n" + (page.extract_text() or "")
            text = s
        except Exception as e2:
            logger.exception("extract_single: fallback pdfplumber failed for %s: %s", p.name, e2)
            return _ensure_minimal_row(p)

    mt = detect_message_type(text)
    if not mt:
        logger.info("%s: MT type not found in text", p.name)
        row = _ensure_minimal_row(p, mt_type=None)
        row["source_pdf"] = p.name
        return row

    extractor = EXTRACTOR_MAP.get(mt)
    if not extractor:
        logger.info("%s: type detected -> %s but no extractor implemented", p.name, mt)
        row = _ensure_minimal_row(p, mt_type=mt)
        row["source_pdf"] = p.name
        return row

    try:
        row = extractor(p)
        if not isinstance(row, dict):
            logger.error("%s: extractor returned non-dict result: %r", p.name, row)
            row = _ensure_minimal_row(p, mt_type=mt)
        else:
            required = ["code_banque", "date_reference", "reference", "type_MT", "pays_iso3",
                        "institution_name", "beneficiaire", "montant", "devise", "source_pdf"]
            for k in required:
                if k not in row:
                    row[k] = None
            if not row.get("institution_name") and row.get("donneur_dordre"):
                row["institution_name"] = row.get("donneur_dordre")
            if not row.get("type_MT"):
                row["type_MT"] = f"fin.{mt}"
            if not row.get("source_pdf"):
                row["source_pdf"] = p.name
        logger.info("%s: extracted via MT%s", p.name, mt)
        return row
    except Exception as e:
        logger.exception("Extraction failed for %s (MT%s): %s", p.name, mt, e)
        row = _ensure_minimal_row(p, mt_type=mt)
        row["error"] = str(e)
        return row


def _sanitize_sheet_title(name: str, max_len: int = 31) -> str:
    """Make a safe Excel sheet name (no invalid chars, limited length)."""
    if not name:
        name = "sheet"
    sanitized = re.sub(r'[:\\\/\?\*\[\]]+', '_', name)
    sanitized = sanitized.strip()
    if len(sanitized) > max_len:
        sanitized = sanitized[:max_len]
    if not sanitized:
        sanitized = "sheet"
    return sanitized


def create_workbook(rows: List[Dict], out_dir: Path) -> Path:
    """
    Create an Excel workbook with:
      - a 'summary' sheet containing one row per extracted file (display headers in French)
      - one additional sheet per file with key/value pairs (debug-friendly)
    Returns the Path to the saved workbook.
    """
    out_dir = Path(out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    out_path = out_dir / f"swift_extraction_{ts}.xlsx"

    wb = Workbook()
    summary = wb.active
    summary.title = "summary"

    # summary headers (user-facing)
    display_headers = [
        "code_banque",
        "date_reference",
        "reference",
        "type_MT",
        "pays_iso3",
        "donneur d'ordre",
        "Bénéficiaire",
        "montant",
        "devise",
        "source_pdf"
    ]
    summary.append(display_headers)

    # write summary rows (map internal keys -> display)
    for r in rows:
        # prefer institution_name, else new key donneur_dordre
        donneur = r.get("institution_name") or r.get("donneur_dordre") or r.get("donneur d'ordre") or None
        beneficiaire = r.get("beneficiaire") or None
        summary.append([
            r.get("code_banque"),
            r.get("date_reference"),
            r.get("reference"),
            r.get("type_MT"),
            r.get("pays_iso3"),
            donneur,
            beneficiaire,
            r.get("montant"),
            r.get("devise"),
            r.get("source_pdf")
        ])

    # create per-file sheets (key/value)
    used_names = set()
    for r in rows:
        base = r.get("source_pdf", "sheet")
        title = _sanitize_sheet_title(str(base))
        original = title
        i = 1
        while title in used_names or title in wb.sheetnames:
            suffix = f"_{i}"
            max_base_len = 31 - len(suffix)
            title = (original[:max_base_len] + suffix) if len(original) > max_base_len else (original + suffix)
            i += 1
        used_names.add(title)
        ws = wb.create_sheet(title=title)

        ordered_keys = [
            "code_banque", "date_reference", "reference", "type_MT", "pays_iso3",
            "institution_name", "beneficiaire", "montant", "devise", "source_pdf"
        ]
        written = set()
        for k in ordered_keys:
            if k in r:
                label = "donneur d'ordre" if k == "institution_name" else ("Bénéficiaire" if k == "beneficiaire" else k)
                ws.append([label, r.get(k)])
                written.add(k)
        for k, v in r.items():
            if k in written:
                continue
            label = "donneur d'ordre" if k == "institution_name" else ("Bénéficiaire" if k == "beneficiaire" else k)
            ws.append([label, v])

        # adjust column widths heuristically
        try:
            max_len_col1 = max((len(str(row[0])) for row in ws.values if row[0] is not None), default=10)
            max_len_col2 = max((len(str(row[1])) for row in ws.values if len(row) > 1 and row[1] is not None), default=10)
            ws.column_dimensions[get_column_letter(1)].width = min(60, max(12, max_len_col1 + 2))
            ws.column_dimensions[get_column_letter(2)].width = min(80, max(12, max_len_col2 + 8))
        except Exception:
            pass

    wb.save(out_path)
    logger.info("Workbook created: %s", out_path)
    return out_path
=== ./backend/app/api.py ===
# backend/app/api.py
from fastapi import APIRouter, Depends, UploadFile, File, HTTPException, status, Form
from fastapi.responses import FileResponse, JSONResponse
from fastapi.security import OAuth2PasswordRequestForm, OAuth2PasswordBearer
from jose import jwt, JWTError
from datetime import timedelta, datetime
from pathlib import Path
import shutil, os
from .db import SessionLocal, init_db, get_user, verify_password
from .db import create_user as create_user_db
from .utils import logger
from .extractor_manager import extract_single, create_workbook
from typing import List

router = APIRouter()
BASE = Path(__file__).resolve().parent.parent
DATA_DIR = BASE / "data"
RAW_DIR = BASE.parent / "data" / "raw"
OUT_DIR = BASE.parent / "output" / "tables"
LOG_FILE = BASE.parent.parent / "logs" / "app.log"

# auth / secrets
SECRET_KEY = os.environ.get("JWT_SECRET", "dev_secret_change_me")
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 60

oauth2_scheme = OAuth2PasswordBearer(tokenUrl="token")

def create_access_token(data: dict, expires_delta: timedelta = None):
    to_encode = data.copy()
    if expires_delta:
        expire = datetime.utcnow() + expires_delta
    else:
        expire = datetime.utcnow() + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    to_encode.update({"exp": expire})
    return jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)

def get_current_user(token: str = Depends(oauth2_scheme)):
    credentials_exception = HTTPException(status_code=401, detail="Could not validate credentials")
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        username: str = payload.get("sub")
        role: str = payload.get("role")
        if username is None:
            raise credentials_exception
    except JWTError:
        raise credentials_exception
    db = SessionLocal()
    user = db.query(__import__("app.db", fromlist=["User"]).User).filter_by(username=username).first()
    db.close()
    if not user:
        raise credentials_exception
    return {"username": username, "role": role}

def require_role(min_role: str):
    order = {"user":0, "admin":1, "superadmin":2}
    def dep(user = Depends(get_current_user)):
        if order.get(user["role"], 0) < order.get(min_role, 0):
            raise HTTPException(status_code=403, detail="Insufficient privileges")
        return user
    return dep

@router.post("/token")
def login(form_data: OAuth2PasswordRequestForm = Depends()):
    db = SessionLocal()
    try:
        u = db.query(__import__("app.db", fromlist=["User"]).User).filter_by(username=form_data.username).first()
        if not u or not verify_password(form_data.password, u.hashed_password):
            raise HTTPException(status_code=400, detail="Incorrect username or password")
        token = create_access_token({"sub": u.username, "role": u.role})
        return {"access_token": token, "token_type": "bearer"}
    finally:
        db.close()

@router.post("/upload")
async def upload(files: List[UploadFile] = File(...), current = Depends(require_role("user"))):
    RAW_DIR.mkdir(parents=True, exist_ok=True)
    OUT_DIR.mkdir(parents=True, exist_ok=True)
    rows = []
    for up in files:
        dest = RAW_DIR / up.filename
        with open(dest, "wb") as f:
            shutil.copyfileobj(up.file, f)
        logger.info(f"Saved upload {up.filename}")
        r = extract_single(dest)
        rows.append(r)
    out_file = create_workbook(rows, OUT_DIR)
    return FileResponse(str(out_file), media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet", filename=out_file.name)

@router.get("/runs")
def list_runs(current = Depends(require_role("admin"))):
    OUT_DIR.mkdir(parents=True, exist_ok=True)
    files = sorted([p.name for p in OUT_DIR.glob("*.xlsx")], reverse=True)
    return {"runs": files}

@router.get("/logs")
def get_logs(skip: int = 0, limit: int = 200, current = Depends(require_role("admin"))):
    if not LOG_FILE.exists():
        return {"lines": []}
    with open(LOG_FILE, "r", encoding="utf-8") as f:
        lines = f.readlines()
    # return last lines
    lines = lines[-(skip + limit):] if (skip+limit) <= len(lines) else lines
    return {"lines": [l.rstrip("\n") for l in lines]}

# bootstrap helper route (only for dev)
@router.post("/bootstrap_users")
def bootstrap_users(current = Depends(require_role("superadmin"))):
    init_db()
    db = SessionLocal()
    # create a default set if not existing
    for u,p,r in [("alice","alicepass","user"),("bob","bobpass","admin"),("root","rootpass","superadmin")]:
        existing = db.query(__import__("app.db", fromlist=["User"]).User).filter_by(username=u).first()
        if not existing:
            create_user_db(db, u, p, r)
    db.close()
    return {"ok": True}
=== ./backend/app/create_users.py ===
# backend/app/create_users.py
#!/usr/bin/env python3
import argparse, sys
from pathlib import Path
HERE = Path(__file__).resolve().parent
sys.path.insert(0, str(HERE))

from db import init_db, SessionLocal, create_user

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("username")
    parser.add_argument("password")
    parser.add_argument("--role", default="user", choices=["user", "admin", "superadmin"])
    args = parser.parse_args()

    init_db()
    db = SessionLocal()
    try:
        u = create_user(db, args.username, args.password, role=args.role)
        print(f"Created user: {u.username} (role={u.role})")
    finally:
        db.close()

if __name__ == "__main__":
    main()
=== ./backend/app/utils.py ===
# backend/app/utils.py
import logging
from pathlib import Path

LOG_DIR = Path(__file__).resolve().parent.parent.parent / "logs"
LOG_DIR.mkdir(parents=True, exist_ok=True)
LOG_FILE = LOG_DIR / "app.log"

def setup_logger():
    logger = logging.getLogger("pdf_extractor")
    if logger.handlers:
        return logger
    logger.setLevel(logging.INFO)
    fh = logging.FileHandler(LOG_FILE, encoding="utf-8")
    formatter = logging.Formatter("%(asctime)s [%(levelname)s] %(message)s")
    fh.setFormatter(formatter)
    logger.addHandler(fh)
    # also console handler
    ch = logging.StreamHandler()
    ch.setFormatter(formatter)
    logger.addHandler(ch)
    return logger

logger = setup_logger()
=== ./backend/app/extractors/bic_utils.py ===
# backend/app/extractors/bic_utils.py
"""
Utilities to extract the IdentifierCode from F52A and to map the first-8-chars
to a human-readable bank name using an Excel table (data/bic_codes.xlsx).

Provides:
 - load_bic_mapping(xlsx_path: Optional[str]) -> Dict[str, str]
 - map_code_to_name(code: str, xlsx_path: Optional[str]) -> Optional[str]
 - get_name_for_code(code: str, xlsx_path: Optional[str]) -> Optional[str]  # alias for compatibility
 - get_donneur_from_f52(f52_text, message_text=None, xlsx_path=None) -> Optional[str]
"""
from pathlib import Path
import re
from functools import lru_cache
from typing import Optional, Dict

try:
    import pandas as pd
except Exception as e:
    raise RuntimeError("pandas is required by bic_utils.py (pip install pandas)") from e

# exact label to find (case-insensitive)
_LABEL_TEXT = "IdentifierCode: Code d'identifiant:"
_LABEL_RE = re.compile(re.escape(_LABEL_TEXT), re.I)

# strict code rule: only uppercase letters, length 8..11
_STRICT_TOKEN_RE = re.compile(r'^[A-Z]{8,11}$')

# fallback token pattern (alphanumeric 8..11) - used only if strict not found
_FALLBACK_TOKEN_RE = re.compile(r'\b([A-Z0-9]{8,11})\b', re.I)

# module-level cache
_BIC_MAP_CACHE: Optional[Dict[str, str]] = None
_BIC_FULLKEY_MAP: Optional[Dict[str, str]] = None


def _find_strict_identifier_in_f52(f52_text: str) -> Optional[str]:
    """
    Apply the strict rule:
      - find exact label (case-insensitive) inside f52_text
      - inspect: same line after label, next line, next-next line
      - return the first token matching ^[A-Z]{8,11}$
    """
    if not f52_text:
        return None

    lines = [ln.rstrip() for ln in f52_text.splitlines()]
    label_idx = None
    label_span_end = None
    for i, ln in enumerate(lines):
        m = _LABEL_RE.search(ln)
        if m:
            label_idx = i
            label_span_end = m.end()
            break

    if label_idx is None:
        return None

    candidates = []

    # same line after label
    same_line_after = lines[label_idx][label_span_end:].strip()
    if same_line_after:
        candidates.append(same_line_after)

    # next two lines (allow blank lines)
    for j in (label_idx + 1, label_idx + 2):
        if j < len(lines):
            candidates.append(lines[j].strip())

    # evaluate candidates strictly: only A-Z length 8..11
    for cand in candidates:
        if not cand:
            continue
        # split candidate into tokens of letters only (strip punctuation)
        toks = re.findall(r'[A-Z]+', cand.upper())
        for t in toks:
            if _STRICT_TOKEN_RE.match(t):
                return t
    return None


@lru_cache(maxsize=1)
def load_bic_mapping(xlsx_path: Optional[str] = None) -> Dict[str, str]:
    """
    Load the BIC mapping Excel file and return a dict mapping 8-char key -> bank name.
    Also populate a full-key map for 11-char exact matches.
    Default path: data/bic_codes.xlsx

    Expected columns: try to detect columns for code and name (flexible).
    """
    global _BIC_MAP_CACHE, _BIC_FULLKEY_MAP
    if _BIC_MAP_CACHE is not None and _BIC_FULLKEY_MAP is not None:
        return _BIC_MAP_CACHE

    fp = Path(xlsx_path) if xlsx_path else Path("data/bic_codes.xlsx")
    mapping: Dict[str, str] = {}
    mapping_full: Dict[str, str] = {}

    if not fp.exists():
        _BIC_MAP_CACHE = {}
        _BIC_FULLKEY_MAP = {}
        return _BIC_MAP_CACHE

    df = pd.read_excel(fp, dtype=str)
    # normalize column names
    cols = [c for c in df.columns]

    # heuristics to find code and name columns
    code_col = None
    name_col = None
    col_upper = [c.strip().upper() for c in cols]

    # common name candidates
    for i, cu in enumerate(col_upper):
        if cu in ("CODE", "BIC", "CODE BIC", "BIC_CODE", "BIC8", "CODE8", "CODE_BIC", "CODEBIC"):
            code_col = cols[i]
            break
    if not code_col:
        # fallback: pick first column whose values look like BICs/8..11 alnum
        for c in cols:
            sample = df[c].dropna().astype(str)
            if not sample.empty and sample.str.match(r'^[A-Z0-9]{6,12}$', case=False).sum() > 0:
                code_col = c
                break

    for i, cu in enumerate(col_upper):
        if cu in ("NOMS", "NOM", "NAME", "BANK", "INSTITUTION", "NOMINSTITUTION"):
            name_col = cols[i]
            break
    if not name_col:
        # fallback: choose the first non-code column
        for c in cols:
            if c != code_col:
                name_col = c
                break

    if not code_col:
        _BIC_MAP_CACHE = {}
        _BIC_FULLKEY_MAP = {}
        return _BIC_MAP_CACHE

    # build mapping
    for _, row in df.iterrows():
        raw_code = str(row.get(code_col) or "").strip()
        if not raw_code:
            continue
        raw_code = raw_code.replace(" ", "").upper()
        raw_name = ""
        if name_col:
            raw_name = str(row.get(name_col) or "").strip()
        # map first 8 chars -> name
        key8 = raw_code[:8]
        if key8:
            if raw_name:
                mapping[key8] = raw_name
            else:
                # if no name column found, use the raw_code as fallback value (rare)
                mapping.setdefault(key8, raw_code)
        # also keep full mapping for exact 11-char keys (useful for BEACCMCX100)
        if len(raw_code) >= 8:
            mapping_full[raw_code] = raw_name or mapping.get(raw_code[:8], "")

    _BIC_MAP_CACHE = mapping
    _BIC_FULLKEY_MAP = mapping_full
    return _BIC_MAP_CACHE


def map_code_to_name(code: str, xlsx_path: Optional[str] = None) -> Optional[str]:
    """
    Map a raw code (8..11 chars) to bank name using loaded mapping.
    Rules:
      - If code exactly matches an 11-char full key present in the sheet (e.g. BEACCMCX100),
        then prefer that exact mapping.
      - Otherwise map using the first 8 characters.
    Returns the bank name or None.
    """
    if not code:
        return None
    _ = load_bic_mapping(xlsx_path=xlsx_path)  # populate caches
    global _BIC_MAP_CACHE, _BIC_FULLKEY_MAP
    code_u = code.strip().upper()
    # prefer exact full key if present
    if _BIC_FULLKEY_MAP and code_u in _BIC_FULLKEY_MAP and _BIC_FULLKEY_MAP[code_u]:
        return _BIC_FULLKEY_MAP[code_u]
    key8 = code_u[:8]
    if _BIC_MAP_CACHE and key8 in _BIC_MAP_CACHE:
        return _BIC_MAP_CACHE[key8]
    return None


# Backwards-compatibility alias expected by older code
def get_name_for_code(code: str, xlsx_path: Optional[str] = None) -> Optional[str]:
    """
    Compatibility wrapper used by some older extractors.
    Returns same as map_code_to_name.
    """
    return map_code_to_name(code, xlsx_path=xlsx_path)


def get_donneur_from_f52(f52_text: Optional[str], message_text: Optional[str] = None, xlsx_path: Optional[str] = None) -> Optional[str]:
    """
    Public helper used by extractors:
     - find strict IdentifierCode token inside f52_text (or within message_text if absent)
     - map to bank name (first 8 chars) and return "CODE/Bank Name"
     - if no mapping, return CODE (the extracted token)
     - return None if no token found

    NOTE: token is expected to be letters A-Z only (8..11) based on your rule.
    """
    code = None
    if f52_text:
        code = _find_strict_identifier_in_f52(f52_text)
    if not code and message_text:
        # try in the full message (cross-page)
        code = _find_strict_identifier_in_f52(message_text)
    # final fallback: search for an 8..11 alpha token inside f52 block
    if not code and f52_text:
        m = _FALLBACK_TOKEN_RE.search(f52_text)
        if m:
            cand = m.group(1).upper()
            # accept only all-letters candidate (user requested only letters as valid code)
            if re.fullmatch(r'[A-Z]{8,11}', cand):
                code = cand

    if not code:
        return None

    name = map_code_to_name(code, xlsx_path=xlsx_path)
    if name:
        return f"{code}/{name}"
    return code
=== ./backend/app/extractors/mt103.py ===
# backend/app/extractors/mt103.py
"""
Extracteur MT103 — wrapper/variant spécifique.
Uses bic_utils.get_donneur_from_f52 for donor mapping.
"""

import re
from pathlib import Path
from typing import Optional
import pdfplumber

from backend.app.extractors.mt202 import (
    get_field_block,
    parse_amount,
    parse_date_YYMMDD,
    detect_country_from_text,
    extract_receiver_bic,
    parse_reference as parse_reference_mt202,
)
from backend.app.extractors.bic_utils import get_donneur_from_f52  # NEW

def parse_f32a_103(text: str) -> dict:
    blk = get_field_block(text, 'F32A') or text
    blk_clean = re.sub(r'#.*?#', '', blk, flags=re.S)
    result = {'date_reference': None, 'devise': None, 'montant': None}
    m_date = re.search(r'(?i)\bDate[:\s]*([0-9]{6})\b', blk_clean)
    if m_date:
        result['date_reference'] = parse_date_YYMMDD(m_date.group(1))
    else:
        m_date2 = re.search(r'(\d{6})', blk_clean)
        if m_date2:
            result['date_reference'] = parse_date_YYMMDD(m_date2.group(1))
    m_cur = re.search(r'(?i)\bDevise[:\s]*([A-Z]{3})\b', blk_clean)
    if m_cur:
        result['devise'] = m_cur.group(1)
    else:
        m_cur2 = re.search(r'(?i)Currency[:\s\S]{0,80}?([A-Z]{3})\b', blk_clean)
        if m_cur2:
            result['devise'] = m_cur2.group(1)
        else:
            m_cur3 = re.search(r'\b([A-Z]{3})\b', blk_clean)
            if m_cur3:
                result['devise'] = m_cur3.group(1)
    candidate = None
    m_line = re.search(r'(?im)^\s*(?:Montant|Amount)\s*[:\-]\s*(.*)$', blk_clean, flags=re.M)
    if m_line:
        line = m_line.group(1).strip()
        nums = re.findall(r'([0-9]+(?:[.,\s][0-9]{1,3})*(?:[.,][0-9]{1,2})?)', line)
        if nums:
            def digits_len(s): return len(re.sub(r'[^0-9]', '', s))
            candidate = max(nums, key=digits_len)
    if not candidate:
        nums_all = re.findall(r'([0-9]+(?:[.,\s][0-9]{1,3})*(?:[.,][0-9]{1,2})?)', blk_clean)
        if nums_all:
            def digits_len(s): return len(re.sub(r'[^0-9]', '', s))
            candidate = max(nums_all, key=digits_len)
    if candidate:
        result['montant'] = parse_amount(candidate)
    return result

def parse_f59_account(text: str) -> Optional[str]:
    blk = get_field_block(text, 'F59') or get_field_block(text, 'F59:')
    if not blk:
        return None
    blk_clean = re.sub(r'#.*?#', '', blk, flags=re.S)
    m = re.search(r'(?m)^\s*\/?([A-Z]{2}[0-9A-Z]{8,34})\b', blk_clean)
    if not m:
        m = re.search(r'\/([A-Z]{2}[0-9A-Z]{8,34})', blk_clean)
    if not m:
        m = re.search(r'([A-Z]{2}[0-9A-Z]{8,34})', blk_clean)
    if not m:
        return None
    candidate = m.group(1)
    candidate_norm = re.sub(r'\s+', '', candidate).upper()
    return candidate_norm

def parse_f52a_or_f50f_institution(text: str) -> Optional[str]:
    """
    Prefer F52A (donor) processed by bic_utils.get_donneur_from_f52.
    If absent, fallback to previous heuristics (F50F/F50).
    """
    # try F52A using strict bic_utils
    f52 = get_field_block(text, 'F52A')
    # If get_donneur_from_f52 returns code/name, use it
    donneur = None
    if f52:
        donneur = get_donneur_from_f52(f52, message_text=text)
        if donneur:
            return donneur

    # fallback: try to get a human-friendly name from F52A (previous logic)
    if f52:
        lines = [l.strip() for l in re.sub(r'<[^>]+>', ' ', f52).splitlines() if l.strip()]
        name_lines = []
        for ln in lines:
            up = ln.upper()
            if up.startswith("IDENTIFIER") or up.startswith("IDENTIFIERCODE") or up.startswith("CODE") or up.startswith("PARTYIDENTIFIER") or up.startswith("IDENTIFIANT"):
                continue
            if re.match(r'^\/[A-Z0-9\/\-]+', ln):
                continue
            if re.fullmatch(r'[A-Z0-9]{6,11}', ln.replace(' ', '')):
                continue
            if len(ln) > 1:
                name_lines.append(ln)
        if name_lines:
            for i, ln in enumerate(name_lines):
                up = ln.upper()
                if 'BANK' in up or 'BANQUE' in up or 'ORABANK' in up:
                    out = ln
                    if i+1 < len(name_lines) and len(name_lines[i+1]) < 40:
                        out = f"{out} / {name_lines[i+1]}"
                    return out.strip()
            out = ' '.join(name_lines[:2]).strip()
            return out

    # fallback to F50F / F50 (client giver)
    blk50 = get_field_block(text, 'F50F') or get_field_block(text, 'F50')
    if blk50:
        lines = [l.strip() for l in blk50.splitlines() if l.strip()]
        name_candidates = []
        for ln in lines:
            up = ln.upper()
            if up.startswith("NAMEANDADDRESS") or up.startswith("DETAILS") or re.search(r'[A-Za-z]', ln):
                if up.startswith("NUMBER") or up.startswith("PARTYIDENTIFIER") or up.startswith("COMPTE"):
                    continue
                if len(ln) >= 4 and re.search(r'[A-Za-z]', ln):
                    name_candidates.append(ln)
        if name_candidates:
            out = ' '.join(name_candidates[:2]).strip()
            return out

    return None

def extract_from_text(text: str, source: str = None) -> dict:
    row = {
        "type_MT": None,
        "code_banque": None,
        "sender_bic": None,
        "receiver_bic": None,
        "reference": None,
        "date_reference": None,
        "devise": None,
        "montant": None,
        "donneur_dordre": None,
        "beneficiaire": None,
        "pays_iso3": None,
        "source_pdf": source
    }
    m_type = re.search(r'\b(?:MT|FIN)[\s\-_]*(\d{3})\b', text, re.I)
    if m_type:
        row["type_MT"] = f"fin.{m_type.group(1)}".lower()
    else:
        row["type_MT"] = "fin.103"
    rb = extract_receiver_bic(text)
    row["code_banque"] = rb
    row["receiver_bic"] = rb
    try:
        ref = parse_reference_mt202(text)
        row["reference"] = ref
    except Exception:
        blk20 = get_field_block(text, 'F20')
        if blk20:
            for ln in blk20.splitlines():
                ln = ln.strip()
                if not ln:
                    continue
                if re.search(r'\d+\/\d+|\w+\/\w+|\d{2,}', ln):
                    row["reference"] = ln
                    break
            if not row["reference"]:
                row["reference"] = blk20.splitlines()[0].strip()
    f32 = parse_f32a_103(text)
    row["date_reference"] = f32.get("date_reference")
    row["devise"] = f32.get("devise")
    row["montant"] = f32.get("montant")

    # F52A or fallback F50F
    inst = parse_f52a_or_f50f_institution(text)
    row["donneur_dordre"] = inst

    # bénéficiaire
    row["beneficiaire"] = parse_f59_account(text)

    # country detection
    row["pays_iso3"] = detect_country_from_text(text)
    return row

def extract_block(block_text: str, source: str = None) -> dict:
    return extract_from_text(block_text, source=source)

def extract_for_mt103(pdf_path):
    txt = ""
    with pdfplumber.open(str(pdf_path)) as pdf:
        for page in pdf.pages:
            txt += "\n" + (page.extract_text() or "")
    return extract_from_text(txt, source=getattr(pdf_path, "name", str(pdf_path)))

if __name__ == "__main__":
    import sys
    from pprint import pprint
    if len(sys.argv) < 2:
        print("Usage: python mt103.py path/to/103.pdf")
        raise SystemExit(1)
    pprint(extract_for_mt103(sys.argv[1]))
=== ./backend/app/extractors/mt202.py ===
# backend/app/extractors/mt202.py
"""
Extracteur MT202 (text-level). Fournit :
- helpers utilitaires (get_field_block, parse_amount, parse_date_YYMMDD, etc.)
- parse_f32a, extract_transaction_reference (robuste)
- expose aussi parse_reference pour compatibilité avec mt103
- utilise bic_utils.get_donneur_from_f52 pour la valeur donneur_dordre (CODE/NAME)
"""

import re
from datetime import datetime
from typing import Optional
from dateutil import parser as dateparser
import pdfplumber

# bic helper (may return "CODE/Name" or "CODE")
try:
    from backend.app.extractors.bic_utils import get_donneur_from_f52, map_code_to_name
except Exception:
    # fallback: define no-op functions if bic_utils missing
    def get_donneur_from_f52(*a, **k):
        return None
    def map_code_to_name(*a, **k):
        return None

# regex / constants
BIC_RE = re.compile(r'\b[A-Z]{4}[A-Z]{2}[A-Z0-9]{2}(?:[A-Z0-9]{3})?\b')
MT_RE = re.compile(r'\b(?:MT|FIN)[\s\-_]*(\d{3})\b', re.I)
CEMAC_MAP = {
    "CM": "CMR", "CMR": "CMR", "CAMEROON": "CMR",
    "GA": "GAB", "GAB": "GAB", "GABON": "GAB",
    "TD": "TCD", "TCD": "TCD", "CHAD": "TCD",
    "CG": "COG", "COG": "COG", "CONGO": "COG",
    "GQ": "GNQ", "GNQ": "GNQ", "EQUATORIAL GUINEA": "GNQ",
    "CF": "CAF", "CAF": "CAF", "CENTRAL AFRICAN REPUBLIC": "CAF"
}

# ---------- PDF text extractor ----------
def extract_text_from_pdf(path):
    txt = ""
    with pdfplumber.open(str(path)) as pdf:
        for page in pdf.pages:
            txt += "\n" + (page.extract_text() or "")
    # minor normalization
    txt = re.sub(r'(?mi)^\s*page\s+\d+\s*(?:of\s*\d+)?\s*$', '', txt, flags=re.M)
    txt = re.sub(r'\r', '\n', txt)
    # collapse excessive blank lines but keep paragraph separation
    txt = re.sub(r'\n{3,}', '\n\n', txt)
    return txt

# ---------- low-level helpers ----------
def get_field_block(text: str, field_label: str) -> Optional[str]:
    """
    Return the multiline text belonging to a tag Fxx (e.g. 'F52A' or 'F20') inside `text`.
    """
    if not text:
        return None
    # Try label with optional trailing colon/description and capture following lines until next Fxx or end
    pattern = re.compile(r'(?si)(' + re.escape(field_label) + r'[:\s]*)(.*?)(?=\nF\d{2}[A-Z]?:|\nF\d{2}\b|$)')
    m = pattern.search(text)
    return m.group(2).strip() if m else None

def parse_amount(s: Optional[str]) -> Optional[float]:
    if not s:
        return None
    s = s.strip()
    # keep digits, thousand separators, decimal separators, minus
    s = re.sub(r'[^\d,.\-\s]', '', s)
    s = s.replace('\xa0', ' ')
    # normalize: detect whether comma is decimal or dot is decimal
    if s.count(',') and s.count('.'):
        # decide by last separator position
        if s.rfind(',') > s.rfind('.'):
            # comma decimal -> remove dots (thousand), replace comma with dot
            s = s.replace('.', '').replace(',', '.')
        else:
            # dot decimal -> remove commas
            s = s.replace(',', '')
    else:
        if s.count(','):
            # comma may be decimal if last group length 1-2 digits
            idx = s.rfind(',')
            if len(s) - idx - 1 in (1, 2):
                s = s.replace('.', '').replace(',', '.')
            else:
                s = s.replace(',', '')
        else:
            # no comma, remove spaces
            s = s.replace(' ', '')
    try:
        return float(s)
    except Exception:
        return None

def parse_date_YYMMDD(s: Optional[str]) -> Optional[str]:
    if not s:
        return None
    s = s.strip()
    if re.fullmatch(r'\d{6}', s):
        yy = int(s[:2]); mm = int(s[2:4]); dd = int(s[4:6])
        year = 2000 + yy
        try:
            return datetime(year, mm, dd).date().isoformat()
        except Exception:
            return None
    try:
        d = dateparser.parse(s, dayfirst=False)
        return d.date().isoformat() if d else None
    except Exception:
        return None

def detect_country_from_text(txt: str) -> Optional[str]:
    if not txt:
        return None
    txtu = txt.upper()
    # try last token of lines
    for line in txtu.splitlines():
        parts = line.strip().split()
        if not parts: continue
        last = parts[-1].strip().strip(',').strip('.')
        if last in CEMAC_MAP:
            return CEMAC_MAP[last]
    # look for longer country name tokens
    for key in CEMAC_MAP:
        if len(key) > 2 and key in txtu:
            return CEMAC_MAP[key]
    # two-letter tokens
    tokens = re.findall(r'\b[A-Z]{2}\b', txtu)
    for t in tokens:
        if t in CEMAC_MAP:
            return CEMAC_MAP[t]
    return None

# ---------- helpers for reference robustness ----------
def _looks_like_amount(s: Optional[str]) -> bool:
    if not s:
        return False
    s_low = s.lower()
    if 'amount' in s_low or 'currency' in s_low or 'montant' in s_low:
        return True
    # detect numbers with thousand separators and decimal comma/dot
    if re.search(r'\b\d{1,3}(?:[.\s]\d{3})*(?:[.,]\d{1,2})\b', s):
        return True
    # detect patterns like "191.700,64" or "191700,64"
    if re.search(r'\d+[.,]\d{2}', s):
        return True
    return False

def extract_transaction_reference(full_text: str, block4_text: Optional[str]) -> Optional[str]:
    """
    Robust extraction of transaction reference.
    Priority:
      1) F20 / :20: inside block4 (handles value on next non-empty line)
      2) header 'Transaction Reference: <TOKEN>' (token = [A-Z0-9_-]{3,})
      3) safe fallback: small token search but avoid picking amounts
    Returns uppercase reference or None.
    """
    # 1) try block4 / F20
    b = block4_text or ""
    if b:
        # same-line pattern: "F20: S065..." or ":20:S065..."
        m = re.search(r'(?mi)^(?:\:20\:|F20[:\s]*)(.*)$', b, flags=re.M)
        if m:
            cand = m.group(1).strip()
            if not cand:
                # find next non-empty line after the matched line
                lines = b.splitlines()
                for i, ln in enumerate(lines):
                    if re.match(r'(?mi)^(?:\:20\:|F20[:\s]*)', ln):
                        j = i + 1
                        while j < len(lines) and not lines[j].strip():
                            j += 1
                        if j < len(lines):
                            cand = lines[j].strip()
                        break
            if cand and not _looks_like_amount(cand):
                tok = re.search(r'([A-Z0-9\-\_]{3,})', cand, flags=re.I)
                if tok:
                    return tok.group(1).upper()
        else:
            # handle label on its own line and value on next line:
            lines = b.splitlines()
            for i, ln in enumerate(lines):
                if re.match(r'(?mi)^\s*(?:F20[:\s]*|:20:)', ln):
                    # see if same-line value
                    same = re.sub(r'(?mi)^\s*(?:F20[:\s]*|:20:)\s*', '', ln).strip()
                    if same:
                        cand = same
                    else:
                        j = i + 1
                        while j < len(lines) and not lines[j].strip():
                            j += 1
                        cand = lines[j].strip() if j < len(lines) else ""
                    if cand and not _looks_like_amount(cand):
                        tok = re.search(r'([A-Z0-9\-\_]{3,})', cand, flags=re.I)
                        if tok:
                            return tok.group(1).upper()
                    break

    # 2) header "Transaction Reference: TOKEN"
    m2 = re.search(r'(?mi)Transaction\s+Reference\s*[:\s]*([A-Z0-9\-\_]{3,})', full_text)
    if m2:
        cand = m2.group(1).strip()
        if not _looks_like_amount(cand):
            return cand.upper()

    # 3) safe fallback: look for a line immediately after "F20" label anywhere in full_text
    m_label = re.search(r'(?mi)(?:F20[:\s]*|:20:)\s*$', full_text, flags=re.M)
    if m_label:
        # find the position, then take next non-empty line
        pos = m_label.end()
        tail = full_text[pos: pos + 400]
        lines = tail.splitlines()
        for ln in lines:
            ln = ln.strip()
            if not ln:
                continue
            if not _looks_like_amount(ln):
                tok = re.search(r'([A-Z0-9\-\_]{3,})', ln, flags=re.I)
                if tok:
                    return tok.group(1).upper()
            break

    # nothing reliable
    return None

# provide parse_reference wrapper for backwards compatibility
def parse_reference(text: str) -> Optional[str]:
    """
    Backwards-compatible wrapper expected by mt103: compute an appropriate block4
    (prefer F20 or Block 4) and call the robust extractor.
    """
    if not text:
        return None
    block4 = get_field_block(text, 'F20') or get_field_block(text, ':20') or get_field_block(text, 'Block 4') or get_field_block(text, 'Block4') or text
    return extract_transaction_reference(text, block4)

# ---------- field parsers ----------
def parse_f32a(text: str) -> dict:
    """
    Parse F32A block (or fallback to text) and return dict with:
    {'date_reference': iso-date or None, 'devise': 'USD'|'EUR'|..., 'montant': float or None}
    """
    blk = get_field_block(text, 'F32A') or text or ""
    blk_clean = re.sub(r'#.*?#', '', blk, flags=re.S)
    res = {'date_reference': None, 'devise': None, 'montant': None}

    # date: try explicit Date: 251222 or a 6-digit token
    m_date = re.search(r'(?i)\bDate[:\s]*([0-9]{6})\b', blk_clean)
    if m_date:
        res['date_reference'] = parse_date_YYMMDD(m_date.group(1))
    else:
        m_date2 = re.search(r'(\d{6})', blk_clean)
        if m_date2:
            res['date_reference'] = parse_date_YYMMDD(m_date2.group(1))

    # currency: try "Currency:" or "Devise:" or any 3-letter token contextually near amount
    m_cur = re.search(r'(?i)\b(?:Devise|Currency)[:\s\S]{0,40}?([A-Z]{3})\b', blk_clean)
    if m_cur:
        res['devise'] = m_cur.group(1).upper()
    else:
        m_cur2 = re.search(r'\b([A-Z]{3})\b', blk_clean)
        if m_cur2:
            res['devise'] = m_cur2.group(1).upper()

    # amount: prefer explicit "Montant|Amount" line
    candidate = None
    m_line = re.search(r'(?im)^\s*(?:Montant|Amount)\s*[:\-]\s*(.*)$', blk_clean, flags=re.M)
    if m_line:
        line = m_line.group(1).strip()
        nums = re.findall(r'([0-9]+(?:[.,\s][0-9]{1,3})*(?:[.,][0-9]{1,2})?)', line)
        if nums:
            def digits_len(s): return len(re.sub(r'[^0-9]', '', s))
            candidate = max(nums, key=digits_len)
    if not candidate:
        # fallback: pick the longest numeric-looking token in block
        nums_all = re.findall(r'([0-9]+(?:[.,\s][0-9]{1,3})*(?:[.,][0-9]{1,2})?)', blk_clean)
        if nums_all:
            def digits_len(s): return len(re.sub(r'[^0-9]', '', s))
            candidate = max(nums_all, key=digits_len)
    if candidate:
        res['montant'] = parse_amount(candidate)
    return res

def extract_receiver_bic(text: str) -> Optional[str]:
    """
    Try to extract the receiver BIC from header 'Receiver:' or anywhere in the text.
    Returns first matched BIC-like token or None.
    """
    if not text:
        return None
    # try 'Receiver:' block
    m = re.search(r'(?i)Receiver\s*[:\-]?\s*(.*?)(?=\n[A-Z][a-z]|$)', text, re.S)
    if m:
        part = m.group(1)
        m2 = BIC_RE.search(part)
        if m2:
            return m2.group(0)
    # fallback: search nearby 'RECEIVER' text region
    idx = text.upper().find('RECEIVER')
    if idx >= 0:
        tail = text[idx: idx + 400]
        m2 = BIC_RE.search(tail)
        if m2:
            return m2.group(0)
    # final fallback: any BIC-looking token in document
    m_any = BIC_RE.findall(text)
    return m_any[0] if m_any else None

# ---------- main extractor for text-block ----------
def extract_from_text(text: str, source: str = None) -> dict:
    row = {
        "type_MT": None,
        "code_banque": None,
        "sender_bic": None,
        "receiver_bic": None,
        "reference": None,
        "date_reference": None,
        "devise": None,
        "montant": None,
        "donneur_dordre": None,
        "beneficiaire": None,
        "pays_iso3": None,
        "source_pdf": source
    }

    # type_MT detection
    m = MT_RE.search(text)
    if m:
        row["type_MT"] = f"fin.{m.group(1)}".lower()

    # receiver BIC (prefer header)
    rb = extract_receiver_bic(text)
    row["code_banque"] = rb
    row["receiver_bic"] = rb

    # robust reference extraction : prefer F20 inside block4 if present
    block4 = get_field_block(text, 'Block 4') or get_field_block(text, 'Block4') or text
    # also try F20 block explicitly
    f20_block = get_field_block(text, 'F20') or get_field_block(text, ':20') or None
    # choose block4_text as f20_block if present else block4
    block4_text = f20_block or block4
    row["reference"] = extract_transaction_reference(text, block4_text)

    # parse amount/date/currency from F32A or text
    f32 = parse_f32a(text)
    row["date_reference"] = f32.get('date_reference')
    row["devise"] = f32.get('devise')
    row["montant"] = f32.get('montant')

    # F52A: use bic_utils.get_donneur_from_f52 to produce CODE/NAME or CODE
    f52_block = get_field_block(text, 'F52A') or get_field_block(text, 'F52A:')
    try:
        donneur = get_donneur_from_f52(f52_block or "", message_text=text)
    except Exception:
        donneur = None
    row["donneur_dordre"] = donneur

    # payer/beneficiary names: try F59/F58 blocks if present (simple best-effort)
    f59 = get_field_block(text, 'F59') or get_field_block(text, 'F58') or None
    if f59:
        # pick first non-empty line as beneficiary readable text
        lines = [ln.strip() for ln in f59.splitlines() if ln.strip()]
        if lines:
            row["beneficiaire"] = lines[0] if not row.get("beneficiaire") else row.get("beneficiaire")

    # country detection
    row["pays_iso3"] = detect_country_from_text(text)

    return row

def extract_block(block_text: str, source: str = None) -> dict:
    return extract_from_text(block_text, source=source)

def extract_for_mt202(pdf_path):
    txt = extract_text_from_pdf(pdf_path)
    return extract_from_text(txt, source=getattr(pdf_path, "name", str(pdf_path)))
=== ./backend/app/extractors/mt910.py ===
# backend/app/extractors/mt910.py
"""
Extractor for SWIFT-like MT910 (confirmations/report outputs).
Rules (as requested):
 - For MT910, donor = sender, beneficiary = receiver.
 - Do NOT consult external BIC mapping for MT910.
 - Extract sender code (exactly 11 chars, A-Z0-9) from Sender Institution block.
 - Extract receiver code (exactly 11 chars) from Receiver Institution block.
 - Extract expansion name from "Expansion:" if present (same block) and produce "[CODE]/[NAME]".
 - If name missing, fall back to a readable line from the block.
 - Always set sender_bic and receiver_bic to the raw 11-char code (if found).
 - Ensure reference extraction still uses header "Transaction Reference" or block4 :20:.
"""

import re
from pathlib import Path
from typing import Optional
import logging

logger = logging.getLogger(__name__)

# reuse helpers from mt202 for consistent text extraction and small utilities
from backend.app.extractors.mt202 import (
    extract_text_from_pdf,
    parse_amount,
    parse_date_YYMMDD,
    detect_country_from_text,
    BIC_RE,
    get_field_block,
)

# ---------- helpers ----------

# strict sender/receiver code: uppercase letters/digits exactly 11 chars
CODE11_RE = re.compile(r'\b([A-Z0-9]{11})\b')

def _parse_block4(text: str) -> Optional[str]:
    m = re.search(r'(?si)Block\s*4(.*?)(?:Block\s*5|Message Text|End of report|End of Message|$)', text)
    return m.group(1).strip() if m else None

def _extract_tag_from_block4(block4: Optional[str], tag: str) -> Optional[str]:
    if not block4:
        return None
    pat = re.compile(r'(?m)^:' + re.escape(tag) + r':\s*(.*)$')
    m = pat.search(block4)
    if m:
        return m.group(1).strip()
    m2 = re.search(r':' + re.escape(tag) + r':\s*([^\r\n]+)', block4)
    return m2.group(1).strip() if m2 else None

def _extract_sender_receiver_header(text: str):
    """
    Extract the raw 'Sender Institution' and 'Receiver Institution' blocks.
    Returns (sender_block_text, receiver_block_text) or (None, None).
    """
    sender = None
    receiver = None
    m_sender = re.search(r'(?si)Sender Institution\s*[:\s]*([^\n].*?)(?=Receiver Institution\s*:|Message Text|Block 4|$)', text)
    if m_sender:
        sender = m_sender.group(1).strip()
    m_receiver = re.search(r'(?si)Receiver Institution\s*[:\s]*([^\n].*?)(?=Message Text|Block 4|$)', text)
    if m_receiver:
        receiver = m_receiver.group(1).strip()
    return sender, receiver

def _compact_whitespace(s: Optional[str]) -> Optional[str]:
    return re.sub(r'\s+', ' ', s).strip() if s else s

def _extract_expansion_name(block_text: str) -> Optional[str]:
    """
    Get the first 'Expansion:' value inside the header block (if present).
    Accepts 'Expansion: NAME' on same line.
    """
    if not block_text:
        return None
    m = re.search(r'(?i)Expansion\s*[:\s]\s*([^\r\n]+)', block_text)
    if m:
        return m.group(1).strip()
    # sometimes "Expansion: <name>" may be followed on next lines; try capture across small window
    m2 = re.search(r'(?i)Expansion\s*[:\s]\s*([^\n]{1,120})', block_text)
    if m2:
        return m2.group(1).strip()
    return None

def _find_code11_in_block(block_text: Optional[str]) -> Optional[str]:
    """
    Find the first token matching exactly 11 alnum (A-Z0-9) in the block.
    Uppercase result returned.
    """
    if not block_text:
        return None
    # search for 11-char token (prefer one containing letters)
    toks = re.findall(r'\b([A-Z0-9]{11})\b', block_text, flags=re.I)
    if not toks:
        return None
    # prefer token that contains a letter (likely BIC-type)
    for t in toks:
        if re.search(r'[A-Z]', t, flags=re.I):
            return t.upper()
    # else return first
    return toks[0].upper()

def _format_code_and_name(code: Optional[str], name: Optional[str]) -> Optional[str]:
    if not code and not name:
        return None
    if code:
        code_u = code.strip().upper()
        if name:
            return f"{code_u}/{_compact_whitespace(name)}"
        return code_u
    # no code but name present
    return _compact_whitespace(name)

# ---------- main extractor ----------

def _extract_from_text(text: str, source: str = None) -> dict:
    row = {
        "type_MT": "fin.910",
        "code_banque": None,
        "sender_bic": None,
        "receiver_bic": None,
        "reference": None,
        "date_reference": None,
        "devise": None,
        "montant": None,
        "donneur_dordre": None,      # legacy key expected by app
        "institution_name": None,    # alias for donneur
        "beneficiaire": None,
        "pays_iso3": None,
        "source_pdf": source,
        "related_reference": None,
        "sender_account": None
    }

    # 0) reference: prefer header "Transaction Reference" (works cross-block) then :20:
    m_tr = re.search(r'(?i)Transaction Reference\s*[:\s]*([A-Z0-9\-\_]+)', text)
    if m_tr:
        row["reference"] = m_tr.group(1).strip()

    # 1) header sender/receiver
    sender_blk, receiver_blk = _extract_sender_receiver_header(text)

    # sender
    if sender_blk:
        code = _find_code11_in_block(sender_blk)
        expansion = _extract_expansion_name(sender_blk)
        formatted = _format_code_and_name(code, expansion)
        if formatted:
            row["donneur_dordre"] = formatted
            row["institution_name"] = formatted
        else:
            # fallback: pick a readable line
            lines = [ln.strip() for ln in sender_blk.splitlines() if ln.strip()]
            if lines:
                # if first token looks like code, remove it from the name
                first = lines[0]
                # if first contains code at start, drop it for name
                mcode = CODE11_RE.search(first)
                if mcode:
                    candidate_name = " / ".join(lines[1:3]) if len(lines) > 1 else first
                else:
                    candidate_name = " / ".join(lines[:2])
                row["donneur_dordre"] = _compact_whitespace(candidate_name)
        # also set sender_bic as code if found
        if sender_blk:
            c11 = _find_code11_in_block(sender_blk)
            if c11:
                row["sender_bic"] = c11
                # set code_banque if not set
                if not row.get("code_banque"):
                    row["code_banque"] = c11

    # receiver
    if receiver_blk:
        code_r = _find_code11_in_block(receiver_blk)
        expansion_r = _extract_expansion_name(receiver_blk)
        formatted_r = _format_code_and_name(code_r, expansion_r)
        if formatted_r:
            row["beneficiaire"] = formatted_r
        else:
            lines = [ln.strip() for ln in receiver_blk.splitlines() if ln.strip()]
            if lines:
                # similar fallback
                first = lines[0]
                mcode = CODE11_RE.search(first)
                if mcode:
                    candidate_name = " / ".join(lines[1:3]) if len(lines) > 1 else first
                else:
                    candidate_name = " / ".join(lines[:2])
                row["beneficiaire"] = _compact_whitespace(candidate_name)
        if code_r:
            row["receiver_bic"] = code_r
            # if code_banque not set prefer receiver
            if not row.get("code_banque"):
                row["code_banque"] = code_r

    # 2) block4 tags: prefer :20: for reference, :32A: for date/currency/amount, :25P for account
    block4 = _parse_block4(text)
    if block4:
        tag20 = _extract_tag_from_block4(block4, '20')
        if tag20:
            row["reference"] = tag20.strip()
        tag21 = _extract_tag_from_block4(block4, '21')
        if tag21:
            row["related_reference"] = tag21.strip()
        tag25 = _extract_tag_from_block4(block4, '25P') or _extract_tag_from_block4(block4, '25')
        if tag25:
            row["sender_account"] = tag25.strip()
        tag32 = _extract_tag_from_block4(block4, '32A')
        if tag32:
            # reuse small parser from mt202: try simple inline parse
            m = re.match(r'^\s*(\d{6})\s*([A-Z]{3})\s*([0-9\.,]+)\s*$', tag32)
            if m:
                date_iso = parse_date_YYMMDD(m.group(1))
                cur = m.group(2).upper()
                try:
                    amt = float(m.group(3).replace('.', '').replace(',', '.'))
                except Exception:
                    amt = None
            else:
                # fallback: find currency token and number
                m2 = re.search(r'([A-Z]{3})\s*([0-9\.,]+)', tag32)
                if m2:
                    cur = m2.group(1).upper()
                    try:
                        amt = float(m2.group(2).replace('.', '').replace(',', '.'))
                    except Exception:
                        amt = None
                else:
                    date_iso = None
                    cur = None
                    amt = None
            if date_iso:
                row["date_reference"] = date_iso
            if cur:
                row["devise"] = cur
            if amt is not None:
                row["montant"] = amt

    # 3) fallback free text amount/date
    if row.get("montant") is None:
        m_amt = re.search(r'(?i)Amount[:\s]*([0-9\.,\s]+)\s*(?:Currency[:\s]*([A-Z]{3}))?', text)
        if m_amt:
            s = m_amt.group(1)
            cur = m_amt.group(2)
            try:
                val = float(s.replace('.', '').replace(',', '.'))
            except Exception:
                val = None
            row["montant"] = val
            if cur:
                row["devise"] = cur.upper()

    if not row.get("date_reference"):
        m_val = re.search(r'(?i)Value Date[:\s]*([0-3]?\d)[\/\-]([01]?\d)[\/\-]([0-9]{2,4})', text)
        if m_val:
            d, mth, y = m_val.group(1), m_val.group(2), m_val.group(3)
            if len(y) == 2:
                y = '20' + y
            try:
                row["date_reference"] = f"{int(y):04d}-{int(mth):02d}-{int(d):02d}"
            except Exception:
                pass

    # 4) country detection
    row["pays_iso3"] = detect_country_from_text(text)

    # normalization: uppercase currency
    if row.get("devise"):
        row["devise"] = row["devise"].upper()

    # cast montant to float if possible (already done)
    try:
        if row.get("montant") is not None:
            row["montant"] = float(row["montant"])
    except Exception:
        row["montant"] = None

    return row

# Public API
def extract_block(block_text: str, source: str = None) -> dict:
    return _extract_from_text(block_text, source=source)

def extract_for_mt910(pdf_path):
    p = Path(pdf_path)
    txt = extract_text_from_pdf(p)
    return _extract_from_text(txt, source=getattr(p, "name", str(p)))

if __name__ == "__main__":
    import sys
    from pprint import pprint
    if len(sys.argv) < 2:
        print("Usage: python3 mt910.py path/to/910.pdf")
        raise SystemExit(1)
    pprint(extract_for_mt910(sys.argv[1]))
=== ./backend/app/extractors/mt_multi.py ===
"""
Dispatcher / découpeur de messages SWIFT.
- Lit un PDF (pdfplumber)
- Découpe en messages
- Détecte le type MT (202, 103, 910, 202.COV, ...)
- Appelle l'extracteur spécialisé (mt202, mt103, mt910)
- Pour 202.COV : utilise la même extraction que 202 pour tous les champs,
  mais met type_MT = "fin.202.COV" (d'après "Identifier: fin.202.COV" du header)
- Post-traitement: pour MT202/MT103 (et variantes), tente d'extraire le token strict
  depuis F52A et de formater "CODE/Bank Name" via bic_utils si disponible.
Returns list[dict] standardisés.
"""

from pathlib import Path
import re
from typing import List, Dict, Optional
import pdfplumber
import logging

logger = logging.getLogger(__name__)

# specialized extractors (block-level API: extract_block(block_text, source=...))
from backend.app.extractors import mt202, mt103, mt910

# optional bic mapping utilities (used only for 202/103 postprocessing)
try:
    from backend.app.extractors import bic_utils
    HAS_BIC_UTILS = True
except Exception:
    bic_utils = None
    HAS_BIC_UTILS = False

# ---------- patterns ----------
# try to capture "Identifier: fin.202.COV" (we will extract the tail e.g. "202" or "202.COV")
IDENTIFIER_FIN_FULL_RE = re.compile(r'(?i)Identifier\s*[:\s]*\s*fin\.(\d{3}(?:\.[A-Z0-9]+)?)')
# fallback simpler inline MT tokens
MT_INLINE_RE = re.compile(r'\b(?:FIN|MT)[\s\-\._:\/]*(\d{3})\b', re.I)

# small helper to get F52A from a block (try to reuse mt202 helper if present)
try:
    from backend.app.extractors.mt202 import get_field_block
except Exception:
    def get_field_block(text: str, field_label: str) -> Optional[str]:
        # crude fallback: find occurrences of the label and return following lines until next F.. or blank
        pat = re.compile(r'(?si)(' + re.escape(field_label) + r'[:\s]*)(.*?)(?=\nF\d{2}[A-Z]?:|\nF\d{2}\b|$)')
        m = pat.search(text)
        return m.group(2).strip() if m else None


def _safe_text_extract(pdf_path: Path) -> str:
    """
    Extract text reliably from pdf using pdfplumber and normalize newlines.
    Keep some whitespace structure (double newlines) but remove excessive blank runs.
    """
    text = ""
    with pdfplumber.open(str(pdf_path)) as pdf:
        for p in pdf.pages:
            text += "\n" + (p.extract_text() or "")
    # normalize
    text = text.replace('\r', '\n')
    # remove "page X of Y" lines often injected
    text = re.sub(r'(?mi)^\s*page\s+\d+\s*(?:of\s*\d+)?\s*$', '', text, flags=re.M)
    # collapse long empty runs to two newlines (keep paragraph separation)
    text = re.sub(r'\n{3,}', '\n\n', text)
    return text


def _split_messages(text: str) -> List[str]:
    """
    Robust splitting into messages. Try multiple heuristics because pdf text extraction
    can vary a lot between files.
    Returns list of message blocks (stripped).
    """
    if not text:
        return []

    txt = text.replace('\r', '\n')
    # keep original to use slices by index
    norm = txt

    # 1) 'Message N' headings (common in many dumps)
    msgs = list(re.finditer(r'(?m)^\s*Message\s+\d+\b', norm))
    if len(msgs) >= 2:
        positions = [m.start() for m in msgs] + [len(norm)]
        blocks = [norm[positions[i]:positions[i+1]].strip() for i in range(len(positions)-1)]
        # filter out empty
        return [b for b in blocks if b]

    # 2) 'Identifier: fin.XXX' header occurrences (covers fin.202.COV etc.)
    idents = list(re.finditer(r'(?mi)Identifier\s*[:\s]*fin\.\d{3}(?:\.[A-Z0-9]+)?', norm))
    if len(idents) >= 2:
        positions = [m.start() for m in idents] + [len(norm)]
        blocks = [norm[positions[i]:positions[i+1]].strip() for i in range(len(positions)-1)]
        return [b for b in blocks if b]

    # 3) 'Unique Message Identifier' / 'Message Identifier' headings
    umi = list(re.finditer(r'(?m)^(?:Unique Message Identifier|Message Identifier)\b', norm, flags=re.M))
    if len(umi) >= 2:
        positions = [m.start() for m in umi] + [len(norm)]
        blocks = [norm[positions[i]:positions[i+1]].strip() for i in range(len(positions)-1)]
        return [b for b in blocks if b]

    # 4) split by :20: / F20 tokens (more tolerant: any occurrence of :20: or F20: or F20)
    token_pat = re.compile(r'(?mi)(:20:|\bF20[:\s])')
    tokens = list(token_pat.finditer(norm))
    if tokens:
        positions = [m.start() for m in tokens]
        if positions and positions[0] != 0:
            positions = [0] + positions
        positions.append(len(norm))
        blocks = [norm[positions[i]:positions[i+1]].strip() for i in range(len(positions)-1)]
        # sometimes splitting on :20: yields an initial tiny prefix; drop very small blocks
        blocks = [b for b in blocks if len(b) > 10]
        if len(blocks) >= 2:
            return blocks

    # 5) visual separators like lines with '***' or '---'
    sep_matches = list(re.finditer(r'(?m)^\s*(\*{3,}|-{3,})\s*$', norm))
    if sep_matches:
        positions = []
        # collect segment between separators
        prev = 0
        blocks = []
        for m in sep_matches:
            s = norm[prev:m.start()].strip()
            if s:
                blocks.append(s)
            prev = m.end()
        tail = norm[prev:].strip()
        if tail:
            blocks.append(tail)
        if len(blocks) >= 2:
            return blocks

    # 6) fallback: try splitting by large page-like separators (multiple underscores)
    page_like = re.split(r'(?m)^\s*_{5,}\s*$', norm)
    if len(page_like) >= 2:
        blocks = [p.strip() for p in page_like if p.strip()]
        if len(blocks) >= 2:
            return blocks

    # final fallback: whole text as single block
    return [norm.strip()]


def _detect_mt_type(block_text: str) -> Optional[str]:
    """
    Detect specific MT type string:
      - prefer Identifier header form -> returns e.g. '202', '202.COV', '910'
      - else fallback to inline MT/FIN token -> returns digits like '202'
    """
    if not block_text:
        return None
    m = IDENTIFIER_FIN_FULL_RE.search(block_text)
    if m:
        return m.group(1)  # e.g. "202" or "202.COV"
    m2 = MT_INLINE_RE.search(block_text)
    if m2:
        return m2.group(1)
    return None


# ---------- postprocessing for 202/103: F52A -> CODE/Name ----------
def _postprocess_row_for_202_103(row: Dict, block_text: str, xlsx_path: Optional[str] = None) -> Dict:
    """
    For MT202 / MT103 and variants (like 202.COV) : attempt to extract a strict Identifier
    token from F52A (or message text) using bic_utils.get_donneur_from_f52 (if available).
    If a CODE or CODE/Name is found, fill row['donneur_dordre'] and row['institution_name'] and
    set row['code_banque'] if missing.
    """
    try:
        f52_block = get_field_block(block_text, 'F52A')
    except Exception:
        f52_block = None

    code_name = None
    code_only = None

    if HAS_BIC_UTILS:
        try:
            # bic_utils.get_donneur_from_f52 returns "CODE/Name" or CODE or None
            code_name = bic_utils.get_donneur_from_f52(f52_block, message_text=block_text, xlsx_path=xlsx_path)
        except Exception as e:
            logger.debug("mt_multi: bic_utils.get_donneur_from_f52 error: %s", e)
            code_name = None

    if not code_name:
        # fallback naive search near label if bic_utils absent or returned None
        m_label = re.search(r'(?i)(?:IdentifierCode|Identifier Code|Code d\'identifiant|Code d identifiant|Identifier code)\s*[:\-\s]*', block_text)
        if m_label:
            tail = block_text[m_label.end(): m_label.end() + 800]
            m_tok = re.search(r'\b([A-Z0-9]{8,11})\b', tail, flags=re.I)
            if m_tok:
                code_only = m_tok.group(1).upper()
                if HAS_BIC_UTILS:
                    try:
                        name = bic_utils.map_code_to_name(code_only, xlsx_path=xlsx_path)
                    except Exception:
                        name = None
                    code_name = f"{code_only}/{name}" if name else code_only
                else:
                    code_name = code_only

    if code_name:
        # store
        if '/' in code_name:
            code_only = code_name.split('/', 1)[0]
        else:
            code_only = code_name
        row["donneur_dordre"] = code_name
        row["institution_name"] = code_name
        if not row.get("code_banque"):
            row["code_banque"] = code_only
    return row


def extract_messages_from_pdf(pdf_path: Path, bic_xlsx: Optional[str] = None) -> List[Dict]:
    """
    Main entrypoint: read pdf_path, split into messages, dispatch to extractors.
    bic_xlsx: optional path forwarded to bic_utils when used in postprocessing.
    """
    pdf_path = Path(pdf_path)
    if not pdf_path.exists():
        raise FileNotFoundError(pdf_path)

    # if bic_utils available, try to preload mapping (best-effort)
    if HAS_BIC_UTILS:
        try:
            bic_utils.load_bic_mapping(bic_xlsx)
        except Exception as e:
            logger.debug("mt_multi: bic mapping preload failed: %s", e)

    text = _safe_text_extract(pdf_path)
    blocks = _split_messages(text)
    multi = len(blocks) > 1
    rows: List[Dict] = []

    for i, blk in enumerate(blocks, start=1):
        source_label = f"{pdf_path.name}#{i}" if multi else pdf_path.name
        mt_type_token = _detect_mt_type(blk)  # e.g. '202', '202.COV', '910'
        row: Optional[Dict] = None

        try:
            if mt_type_token and mt_type_token.startswith('202'):
                # includes '202' and variants like '202.COV'
                row = mt202.extract_block(blk, source=source_label)
                # postprocess like other 202/103
                row = _postprocess_row_for_202_103(row, blk, xlsx_path=bic_xlsx)

                # FORCE beneficiary empty for 202 variants (requirement)
                try:
                    row["beneficiaire"] = None
                except Exception:
                    row.update({"beneficiaire": None})

                # if variant .COV present, force type_MT accordingly
                if '.' in mt_type_token:
                    # example: mt_type_token == '202.COV' -> type_MT 'fin.202.COV'
                    row['type_MT'] = f"fin.{mt_type_token}"
                else:
                    row.setdefault('type_MT', 'fin.202')

            elif mt_type_token == '103':
                row = mt103.extract_block(blk, source=source_label)
                row = _postprocess_row_for_202_103(row, blk, xlsx_path=bic_xlsx)
            elif mt_type_token == '910':
                # For 910 we do NOT use bic mapping in dispatcher; mt910 is responsible
                row = mt910.extract_block(blk, source=source_label)
            else:
                # unknown: try mt202 then mt103 then mt910 as fallbacks (keeps existing behavior)
                try:
                    row = mt202.extract_block(blk, source=source_label)
                    row = _postprocess_row_for_202_103(row, blk, xlsx_path=bic_xlsx)
                except Exception:
                    try:
                        row = mt103.extract_block(blk, source=source_label)
                        row = _postprocess_row_for_202_103(row, blk, xlsx_path=bic_xlsx)
                    except Exception:
                        try:
                            row = mt910.extract_block(blk, source=source_label)
                        except Exception:
                            row = {
                                "type_MT": None,
                                "code_banque": None,
                                "sender_bic": None,
                                "receiver_bic": None,
                                "reference": None,
                                "date_reference": None,
                                "devise": None,
                                "montant": None,
                                "donneur_dordre": None,
                                "beneficiaire": None,
                                "pays_iso3": None,
                                "source_pdf": source_label
                            }
        except Exception as e:
            logger.exception("mt_multi: extractor failed for message %s (detected=%s): %s", source_label, mt_type_token, e)
            row = {
                "type_MT": f"fin.{mt_type_token}" if mt_type_token else None,
                "code_banque": None,
                "reference": None,
                "date_reference": None,
                "devise": None,
                "montant": None,
                "donneur_dordre": None,
                "beneficiaire": None,
                "pays_iso3": None,
                "source_pdf": source_label,
                "error": str(e)
            }

        # ensure expected keys present
        expected = ["type_MT","code_banque","sender_bic","receiver_bic","reference","date_reference",
                    "devise","montant","donneur_dordre","beneficiaire","pays_iso3","source_pdf"]
        for k in expected:
            if k not in row:
                row[k] = None
        if not row.get("source_pdf"):
            row["source_pdf"] = source_label

        rows.append(row)

    return rows


# quick CLI for manual test
if __name__ == "__main__":
    import sys
    from pprint import pprint
    if len(sys.argv) < 2:
        print("Usage: python mt_multi.py path/to/all.pdf")
        raise SystemExit(1)
    path = Path(sys.argv[1])
    pprint(extract_messages_from_pdf(path))
=== ./scripts/debug_extract_f52_codes.py ===
# scripts/debug_extract_f52_codes.py
# Usage: python scripts/debug_extract_f52_codes.py path/to/all.pdf
import sys
from pathlib import Path
import re
from backend.app.extractors import mt_multi as mtm
from backend.app.extractors.mt202 import get_field_block, parse_reference

# Robust extractor: find 8-11 alnum token after "IdentifierCode" label, allowing newline
IDENTIFIER_AFTER_LABEL_RE = re.compile(
    r"(?i)(?:IdentifierCode|Identifier Code|Identifiercode|Code d'identifiant|Code d identifiant|IDENTIFIERCODE)\s*[:\-\s]*\n?\s*([A-Z0-9]{8,11})",
    re.M
)

# labels that are false-positives (11 letters) we want to avoid
_BAD_LABEL_PREFIXES = ("IDENTIF", "PARTYIDENT", "PARTYIDENTI")

def extract_raw_identifier_from_block(f52_text: str, message_text: str = None):
    """
    Retourne le token brut (8..11 alnum) associé à IdentifierCode si trouvé.
    Recherche dans f52_text d'abord, puis dans message_text (cross-page).
    Filtre tokens qui ressemblent à des étiquettes (IDENTIFIERC, PARTYIDENTI...).
    """
    if not f52_text and not message_text:
        return None

    txt_f52 = (f52_text or "").replace('\r', '\n')
    # 1) recherche stricte dans F52A (label + token possibly next line)
    m = IDENTIFIER_AFTER_LABEL_RE.search(txt_f52)
    if m:
        tok = m.group(1).upper()
        # reject obvious label-like tokens
        if not any(tok.startswith(pref) for pref in _BAD_LABEL_PREFIXES):
            return tok

    # 2) recherche dans le message complet (permet cross-page)
    if message_text:
        full = message_text.replace('\r', '\n')
        m2 = IDENTIFIER_AFTER_LABEL_RE.search(full)
        if m2:
            tok = m2.group(1).upper()
            if not any(tok.startswith(pref) for pref in _BAD_LABEL_PREFIXES):
                return tok
        # sometimes label occurs alone and token is on next non-empty line
        m_label = re.search(r"(?i)(?:IdentifierCode|Code d'identifiant|Identifiercode)\s*[:\-\s]*\n?", full)
        if m_label:
            tail = full[m_label.end(): m_label.end() + 500]  # lookahead window
            m3 = re.search(r"\b([A-Z0-9]{8,11})\b", tail)
            if m3:
                tok = m3.group(1).upper()
                if not any(tok.startswith(pref) for pref in _BAD_LABEL_PREFIXES):
                    return tok

    # 3) final fallback: any 8-11 token inside F52A (but avoid label-like)
    if txt_f52:
        m4 = re.search(r"\b([A-Z0-9]{8,11})\b", txt_f52)
        if m4:
            tok = m4.group(1).upper()
            if not any(tok.startswith(pref) for pref in _BAD_LABEL_PREFIXES):
                return tok

    return None

def main(pdf_path: Path):
    if not pdf_path.exists():
        print("File not found:", pdf_path)
        return

    # read whole text and split messages using existing logic
    text = mtm._safe_text_extract(pdf_path)
    blocks = mtm._split_messages(text)
    print(f"Messages detected: {len(blocks)}\n")

    for i, blk in enumerate(blocks, start=1):
        # reference (best-effort)
        try:
            ref = parse_reference(blk) or "(no-ref)"
        except Exception:
            ref = "(no-ref)"
        # extract raw F52A block using helper (reuse mt202.get_field_block)
        f52 = get_field_block(blk, 'F52A') or get_field_block(blk, 'F52A:')
        # also attempt generic mt_multi fallback if present
        if not f52:
            try:
                f52 = mtm._extract_field_block('F52A', blk)
            except Exception:
                f52 = None

        raw_code = extract_raw_identifier_from_block(f52, message_text=blk)
        print(f"Message #{i} | ref={ref} | raw_identifier={raw_code}")
        print("----- F52A (first 5 lines) -----")
        if f52:
            for ln in (f52.splitlines()[:5]):
                print("  ", ln)
        else:
            print("   <no F52A found>")
        print("\n")
    print("Done.")

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python scripts/debug_extract_f52_codes.py path/to/all.pdf")
        raise SystemExit(1)
    pdf = Path(sys.argv[1])
    main(pdf)
=== ./scripts/extract_f52_strict.py ===
# scripts/extract_f52_strict.py
# Usage: python scripts/extract_f52_strict.py path/to/all.pdf
import sys
import re
from pathlib import Path

# import helpers from your project
try:
    from backend.app.extractors import mt_multi as mtm
    from backend.app.extractors.mt202 import get_field_block, parse_reference
except Exception as e:
    # fallback: try to import with project root on PYTHONPATH
    raise

LABEL = "IdentifierCode: Code d'identifiant:"  # match this (case-insensitive)
LABEL_RE = re.compile(re.escape(LABEL), re.I)

# token rule: only uppercase letters A-Z, length 8..11
TOKEN_RE = re.compile(r'^[A-Z]{8,11}$')

def find_strict_identifier_in_f52(f52_text: str):
    """
    Strict rule:
      - find line that contains the label text (case-insensitive)
      - examine same line (content after the label), next line, and next-next line
      - return the first token that matches ^[A-Z]{8,11}$
    """
    if not f52_text:
        return None

    lines = [ln.rstrip() for ln in f52_text.splitlines()]
    # normalize lines by stripping trailing/leading whitespace only for checks
    norm_lines = [ln for ln in lines]  # keep original for printing

    # find the index of line that contains the label
    label_idx = None
    label_span_pos = None
    for i, ln in enumerate(lines):
        if LABEL_RE.search(ln):
            label_idx = i
            # record position within line to consider "same line after label"
            m = LABEL_RE.search(ln)
            label_span_pos = m.end()
            break

    # if label not found inside block, return None
    if label_idx is None:
        return None

    # candidates: same line remainder, next line, next+1 line (in that order)
    candidates = []

    # same line after label: extract substring after label position
    same_line = lines[label_idx]
    after = same_line[label_span_pos:].strip()
    if after:
        candidates.append(after)

    # next lines (allow empty lines: we will skip empties when checking but must allow one empty line)
    for j in (label_idx + 1, label_idx + 2):
        if j < len(lines):
            candidates.append(lines[j].strip())

    # Evaluate candidates: we accept only tokens fully alphabetic uppercase, 8..11 chars
    for cand in candidates:
        if not cand:
            continue
        # sometimes lines contain other words; split tokens by whitespace/punctuation
        toks = re.findall(r'[A-Z0-9]+', cand.upper())
        for t in toks:
            if TOKEN_RE.match(t):
                return t  # strict: letters only length 8..11

    # nothing found
    return None

def main(pdf_path: Path):
    if not pdf_path.exists():
        print("File not found:", pdf_path)
        return

    # extract full text & split messages using your mt_multi helper
    text = mtm._safe_text_extract(pdf_path)
    blocks = mtm._split_messages(text)
    print(f"Messages detected: {len(blocks)}\n")

    for i, blk in enumerate(blocks, start=1):
        # try to get reference (F20) for display
        try:
            ref = parse_reference(blk) or "(no-ref)"
        except Exception:
            ref = "(no-ref)"

        # get F52A using mt202.get_field_block (fallback to mt_multi internal if needed)
        f52 = None
        try:
            f52 = get_field_block(blk, 'F52A')
        except Exception:
            pass
        if not f52:
            try:
                f52 = mtm._extract_field_block('F52A', blk)
            except Exception:
                f52 = None

        # show first lines and attempt strict extraction
        code = find_strict_identifier_in_f52(f52 or "")

        print(f"Message #{i} | ref={ref} | strict_code={code}")
        print("----- F52A (first 8 lines) -----")
        if f52:
            for ln in f52.splitlines()[:8]:
                print("  ", repr(ln))
        else:
            print("   <no F52A block found>")
        print("\n")

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python scripts/extract_f52_strict.py path/to/all.pdf")
        raise SystemExit(1)
    main(Path(sys.argv[1]))
